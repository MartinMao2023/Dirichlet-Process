\documentclass[journal]{IEEEtran}
%\documentclass{article}
%\usepackage[margin=1in]{geometry}
%\usepackage{multicol}


\title{Collaborative Learning with Dirichlet Process Clustering for Rapid Online Adaptation in Robotics}
\author{Runjun Mao \\ August 12, 2023 \\ Supervised by Antoine Cully}
%\date{August 12, 2023}


\usepackage{bm}
%\usepackage{amssymb}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\usepackage{hyperref}
\hypersetup{hidelinks}
%\usepackage[colorlinks, linkcolor=blue, citecolor=green, urlcolor=red]{hyperref}


\begin{document}
\maketitle

\begin{abstract}
\noindent
Robots in deployment can face unforeseeable distortion in their motion due to unseen environment or hardware failures. 
Model-based online adaptation learns this distortion from the interactions with the current environment and corrects its motions accordingly. However, this can be very challenging as it is hard to select the most suitable model and its hyper-parameters for the current dynamics with little prior knowledge.
If we could leverage the plentiful historical real-world interactions, we may build statistical models for the common types of dynamics faced by the robot, hence providing guidance for adaptations. 
This paper developed a method to enable fast online adaptation for the robot by grouping up historical  interaction data into clusters. 
The historical data can be collected from several robots of the same design performing different tasks in different environments, hence allowing learning in a collaborative manner. 
The method takes an non-parametric approach of novelty by modelling the data distribution with infinite mixture of Gaussian Processes and conducting clustering using Dirichlet Processes. 
This achieves unbiased training while ensures high efficiency in terms of both data and computation during deployment. 
The experiment is conducted on a simulated robot to compare with the previous work for online adaptation. 
The method accurately groups up the collaboratively collected data without any knowledge of their sources, and it offers a significant improvement over the baseline in both the efficiency of learning the distortion and the capability of adaptation. 

\end{abstract}


%\begin{multicols}{2}
\input{introduction.tex}


\section{Related Works}
\subsection{Quality-Diversity Optimization}
The repertoire-based method uses quality diversity (QD) optimization to generate a repertoire of diversified and high-quality locomotion behaviours. 
The most popular method for doing QD is called MAP-Elites \cite{QD, Map-Elites}, where we tabularize the task-space (for example, discretize the displacement of the robot along the x, y axis) into grids. 
Each grid corresponds to a policy that leads the robot to this part of task space. 
Initially the grids are empty, as we have no policies at start, and will be filled during a process of stochastic optimization inspired by natural evolutionary. 
We start from a few random policies, which are typically neural networks, and execute them in the simulator to get their outcomes in the task space. 
We also assign a performance score (called fitness) to each policy to quantify the efficiency of the execution. Typically, the fitness can be based on the energy consumption \cite{Q-Dax} or the shape of the trajectory \cite{evorbc_conf}.
Each of these outcomes is mapped to one of the girds (hence these grids are filled). 
We then begin our main loop of policy generation. 
In each iteration we select a few policies corresponding to the filled grids as the parents. We then conduct crossover on the parents to mutate their genotypes (for neural networks, these are just the weights of the neurons) to get their off-springs. 
The crossover operation should balance the exploitation and exploration by ensuring some inheritance of the parents to preserve good genes as well as some random variations for promoting diversity.
We then evaluate the off-springs in the simulator to get their outcomes in the task space and identify the grids they belong to.
If a grid is already occupied by another policy, we only keep the one with the higher fitness; if it is empty, we fill this grid with the corresponding off-spring. 
The pseudo code for MAP-Elites QD algorithm is presented below.

\begin{algorithm}
\caption{MAP-Elites}
\begin{algorithmic}
\STATE \textbf{procedure} MAP-Elites
\STATE Discretize the task space into grids
\STATE $\mathcal{R} \leftarrow \emptyset$ \COMMENT{Initialize repertoire}
\FOR{$\bm{i} = 1 \rightarrow G$}
\STATE $\theta \leftarrow $ generate{\_}random{\_}policy()
\STATE $\Theta$, $f$ $\leftarrow$ evaluate{\_}policy($\theta$) \COMMENT{Get outcome and fitness}
\STATE add{\_}to{\_}repertoire($\Theta$, $f$, $\theta$, $\mathcal{R}$) \COMMENT{Update repertoire}
\ENDFOR
\FOR{$\bm{i} = 1 \rightarrow T$}
\STATE $\vartheta \leftarrow$ select{\_}parents($\mathcal{R}$)
\STATE $\bm{\theta} \leftarrow$ crossover($\vartheta$, $N$) \COMMENT{Generate $N$ off-springs}
\FOR{$\bm{j} = 1 \rightarrow N$}
\STATE $\Theta$, $f$ $\leftarrow$ evaluate{\_}policy($\bm{\theta_j}$)
\COMMENT{Off-spring evaluation}
\STATE add{\_}to{\_}repertoire($\Theta$, $f$, $\bm{\theta_j}$, $\mathcal{R}$)
\ENDFOR
\ENDFOR
\end{algorithmic}
\label{MAP-Elites}
\end{algorithm}
\noindent
Evaluating the policies in the simulation is the major source of computationally expense for the MAP-Elites.
A merit of this algorithm is that the inner for loop of off-spring evaluation can be conducted in parallel, and the repertoire update can be made after having collected all the results of the off-springs.
This enables us to generate a large number of off-springs in each iteration and evaluate them in parallel, fully leveraging the power of modern Cluster computing \cite{Q-Dax}.
By running the algorithm for a few thousand iterations, we will end up having a repertoire of high-performing policies well-covering the task space.

\subsection{Gaussian Process}
The Gaussian Process (GP) is a powerful non-parametric machine learning algorithm. 
It assumes the prior distribution of the outcomes to be a multivariate normal distribution with a prior mean $\mu$ and a covariance matrix $\Sigma$. 
\begin{equation}
f(\bm{y}) = \frac{exp(
-\frac{1}{2}(\bm{y} - \bm{\mu})^T \bm{\Sigma^{-1}} (\bm{y} - \bm{\mu})
)}
{
\sqrt{(2\pi)^N \det(\bm{\Sigma})}
}
\label{multi_normal}
\end{equation}
In contrast to regular multivariate normal distribution, GP allows the prior mean and the covariance matrix to be input dependent. The
prior mean becomes a function $\mu(\bm{x})$, and the covariance between two data points is given by a covariance function 
$k(\bm{x_1}, \bm{x_2})$ called kernel. 
Following the fact that data points closer to each other are more correlated, this covariance starts from the prior variance and asymptotically decreases to zero as the distance between the two points tends to infinity. 
To account for the fact the observations might be noisy, we can incorporate the noise into the covariance matrix by adding the variance of the noise on the diagonal.
\begin{equation}
\bm{\Sigma} = \bm{K}(\bm{x_{1:N}}, \bm{x_{1:N}}) + \sigma_n^2 \mathbf{I}
\label{covariance_matrix}
\end{equation}

This allows us to build reasonable joint prior distribution for the values of any well-behaved functions.
To make prediction with GP, we first use the prior mean function and kernel to build up the joint distribution as the prior. 
Then we can calculate the posterior for the data we wish to evaluate conditioned on the ones we have observed using Bayes rule.
Since the Gaussian distribution is a conjugate distribution, the posterior is still a Gaussian with mean and variance given by \cite{GP, GP_posterior}:
\begin{equation}
\begin{gathered}
\mu_*(x) = \mu(x) + 
\bm{\Sigma}_{N, x}^T
\bm{\Sigma}_{N, N}^{-1}
(\bm{y}_{1:N} - \bm{\mu}(\bm{x}_{1:N}))
\\
\sigma^2_*(x) = \sigma^2(x) -  
\bm{\Sigma}_{N, x}^T
\bm{\Sigma}_{N, N}^{-1}
\bm{\Sigma}_{N, x}
\end{gathered}
\label{GP_posterior}
\end{equation}
where $\bm{\Sigma}_{N, N}$ denotes the covariance matrix of the observed data, and $\bm{\Sigma}_{N, x}$ denotes the column vector with entry on each row equal to the covariance between 
each observed data and the data we hope to evaluate.

Typically, the prior mean is just a fixed constant like zero, representing the prior knowledge we have about the outcome when there is no neighbouring data to refer to.
The typical choice of kernel can be RBF kernel that models the correlations to decrease in the form of a Gaussian function.
\begin{equation}
k(\bm{x_i}, \bm{x_j}) = \alpha_0 e^{-\frac{1}{2} d_{ij}^2}
\label{RBF_kernel}
\end{equation}
RBF kernel assumes the function is infinitely differentiable, which may be factually incorrect. Hence, it might be more desirable to use the Matern kernel:
\begin{equation}
k(\bm{x_i}, \bm{x_j}) = \alpha_0 \frac{2^{1-v}}{\Gamma(v)}
(\sqrt{2v} d_{ij})^v K_v(\sqrt{2v} d_{ij})
\label{Matern}
\end{equation}
where the $\Gamma$ is the gamma function, and the $K_v$ is the modified Bessel function of the second kind. The $v$ controls the smoothness, as GP using Matern kernel is $\lceil v \rceil - 1$ times differentiable in the mean-square sense \cite{GP, Matern}.
The distance $d_{ij}$ between two data points does not have to be Euclidean distance. A more general form would be:
\begin{equation}
d_{ij}^2 = \Sigma_{d=1}^D \frac{(x_{d, i} - x_{d, j})^2}{l_d^2}
\label{distance_metric}
\end{equation}
Where this distance metric can be anisotropic, suggesting distance along some axis contributes more uncertainty than others. 
The $l_d$ in the denominator is a positive number called length scale that determines how slowly we lose uncertainty (bigger length scale means the function is flatter) along the $d^\text{th}$ axis.

\subsection{Reset-free Trial-and-Error Learning}
The work of this paper is based on the Reset-free Trial-and-Error \cite{RTE} (RTE) algorithm. 
The RTE uses GP to learn the transformation model online.
This is a very clever choice for two reasons.
First, the online learning requires very high data-efficiency, which happens to be one of the strengths of GP. 
Secondly, the true dynamics happens at low level, while the repertoire-based method learns the transformation model at high level. This will inevitably introduce errors, which can be well incorporated by GP as being a probabilistic model.
In RTE, the task space is just the displacement of the robot in x and y direction after executing each policy.
The prior mean function of RTE is chosen to be the outcomes of the policies evaluated in the simulator, which are the results collected during the MAP-Elites policy generation. 
Since the policy outcome has two dimensions (x and y), RTE uses a different GP for each dimension.
The kernel is chosen to be the RBF kernel with an isotropic distance metric. The input space is chosen to be the same as the task space, namely the x and y displacement evaluated in the simulation. 
During online adaptation, the RTE first predicts the outcome of each policy in the repertoire using Eq. (\ref{GP_posterior}), then it uses a Monte Carlo Tree Search \cite{MCTS} (MCTS) to plan its actions.
It is worth noting that the RTE uses a periodic set of commands as the elementary policy. 
This means the policy execution is open-loop control, despite the robot knows its position and orientation.
While using such periodic controllers seems to be weaker than using a neural network, it is much more stable than using neural networks. 
This is because a periodic controller performs all its actions during evaluation, while neural networks are much more complicated and could behave very differently in different situations.
Since the repertoire based control aims to learn the distortion with limited number of interactions, the distortion cannot be too large or too complicated. Hence the use of periodic controller is a much better choice. 


This method is not very ideal for three reasons. 
First, the use of outcomes in default simulation setting as the prior mean is not good enough. Since some conditions like broken leg can lead to very large distortions. In this case, such prior would be misleading. 
Second, the use of the task space as the input space is not ideal. Two policies leading to very close displacements could follow very different trajectories, hence they may react very differently to the same new environment.
Also, the kernel in the RTE is manually chosen, which depends on experiences of the designer and could lead to suboptimal results.
Nevertheless, this method successfully enabled a damaged hexapod robot to recover 77.52\% of its capability \cite{RTE} and significantly outperforms the baseline result that does not use the GP as the transformation model.
Some other works like Adaptive Prior selection for Repertoire-based Online Learning \cite{APROL} (APROL) take very similar approach as RTE but use several repertoires generated in different environments to address potential large distortions. 
Since RTE is easier to implement, and any improvements on the RTE can be easily transferred to other other works, it is selected as the algorithm that our work is based on.

\subsection{Dirichlet Process Mixture Model}
Dirichlet process mixture model (DPMM), also called infinite mixture model, is a Bayesian non-parametric model widely used in data clustering. 
It is based on a stochastic process called Dirichlet Process (DP).
DP assumes that the data are sampled from a distribution of distributions. For example, the data points come from a set of Gaussian distributions, while the mean and variance of each Gaussian follow another distribution. 
To make clear explanation, we first investigate the finite mixture model using DP, and then extend the derivations to the infinite limit.
We know that finite mixture models can be represented as:
\begin{equation}
p(\bm{y}) = \Sigma_{j=1}^k \pi_j \cdot p(\bm{y}|\bm{\theta_j})
\label{finite_mixture_model}
\end{equation}
where this model consists of $k$ components, and $\bm{\theta_j}$ and $\pi_j$ are the parameters and the mixture weights (also called mixing proportions) for each mixture component.
DP assumes the parameters and the mixture weights are independently sampled. 
The parameters of the mixture components are sampled from a base distribution $H$, and the mixture weights are sampled from a symmetric Dirichlet distribution \cite{DP}:
 \begin{equation}
p(\pi_1, \dots, \pi_k | \alpha) = 
\frac{\Gamma(\alpha)}{\Gamma(\alpha / k)^k} \prod_{j=1}^k \pi_j^{a/k - 1}
\label{Dirichlet_distribution}
\end{equation}
Where the mixture weights are positive and sum up to 1. The constant $\alpha$ is called concentration parameter, which controls how dense the sampling will be.
To train a DPMM, the only thing we need to determine is the indicator of each data which points to the mixture component that this data belongs to. 
We then try to find the configuration of the indicators that maximizes the posterior likelihood (MAP estimate). 
This is very hard to do directly, typical alternative approaches include Gibbs sampling \cite{Gibbs_sampling, Gibbs} and the use of variational inference \cite{variational_method}.
This paper uses the Gibbs sampling method. 
In each iteration, we loop through each indicator and resample it based on the other indicators:
\begin{equation}
\begin{gathered}
p(c_i = j|y_i, \bm{y}_{-i}, \bm{c}_{-i}, \alpha) 
\propto  \\
p(c_i = j|\bm{c}_{-i}, \alpha) \cdot
p(y_i|c_i=j, \bm{y}_{-i}, \bm{c}_{-i})
\end{gathered}
\label{indicator_posterior}
\end{equation}
%
The subscript $-i$ means all the data except for $i$.
We know the probability of getting a certain configuration is:
\begin{equation}
p(c_1,\dots, c_n|\pi_1, \dots, \pi_k) = \prod_{j=1}^k \pi_j^{n_j}
\label{configuration_probability}
\end{equation}
Where the $n_j$ in the superscript denotes the number of data assigned to the $j^\text{th}$ component.
Using Eq. (\ref{Dirichlet_distribution}) and standard Dirichlet integral, we can calculate the probability density of such configuration in the prior:
\begin{equation}
\begin{gathered}
p(c_1, \dots, c_n | \alpha) =  \frac{\Gamma(\alpha)}
{\Gamma(\alpha / k)^k} \int \prod_{j=1}^k \pi_j^{n_j + a/k - 1} d\bm{\pi}
\\
= \frac{\Gamma(\alpha)}{\Gamma(\alpha + n)} 
\prod_{j=1}^k \frac{\Gamma(n_j + \alpha/k)}{\Gamma(\alpha/k)}
\end{gathered}
\label{indicator_prior}
\end{equation}
Hence we can find the posterior using Bayes rule:
\begin{equation}
p(c_i =j | \bm{c}_{-i} , \alpha) = 
\frac{n_{-i, j} + \alpha /k}{\alpha + n - 1}
\label{indicator_posterior_2}
\end{equation}
%
So far, we have been discussing the case of finite mixture models. If we let the component number $k$ tend to infinity, we will have the equations for infinite mixture models:
\begin{equation}
\begin{gathered}
p(c_i =j | \bm{c}_{-i} , \alpha) = 
\frac{n_{-i, j}}{\alpha + n - 1}
\\
p(c_i \neq j \, \text{for any} \, n_j \neq 0 | \bm{c}_{-i} , \alpha) = 
\frac{\alpha}{\alpha + n - 1}
\end{gathered}
\label{indicator_posterior_3}
\end{equation}
For any existing cluster (mixture component with at least one data assigned to it), this probability is proportional to the number of data in that cluster. 
Note that there is a non-zero probability that this data belongs to a new cluster. 
This is a very important property of DPMM that new clusters can be automatically generated based on the likelihood. 
To calculate the last term in Eq. (\ref{indicator_posterior}), we integrate the likelihood over the posterior:
\begin{equation}
\begin{gathered}
p(y_i|c_i, \bm{y}_{-i}, \bm{c}_{-i}) = 
\int p(y_i|\bm{\theta})
p(\bm{\theta}|c_i, \bm{y}_{-i}, \bm{c}_{-i})
d\bm{\theta}
%
\\
%
\text{where \,\,} 
p(\bm{\theta}|c_i, \bm{y}_{-i}, \bm{c}_{-i}) \propto
\,\, p(\bm{\theta})
\prod_{c_k=c_i, k \neq i} p(y_k|\bm{\theta})
\end{gathered}
\label{integral_1}
\end{equation}
In the case of a new cluster, the integration is made over the prior:
\begin{equation}
p(y_i|c_i \neq j \, \text{for any} \, n_j \neq 0) = 
\int p(y_i|\bm{\theta})
p(\bm{\theta})d\bm{\theta}
\label{integral_2}
\end{equation}
To conduct Gibbs sampling, we first start from an initial configuration of indicators (common choice is that every data is a cluster on its own).
Then in each iteration, we loop through each indicator and use Eq. (\ref{indicator_posterior}) to resample it. 
The $-i$ subscript on $\bm{y}$ means we need to remove the data from its current cluster during resampling, which will affect the result of Eq. (\ref{integral_1}) when calculating the probability of the data remaining in its current cluster. 
The Gibbs sampling is a Markov Chain Monte Carlo (MCMC) that satisfies irreducibility, positive recurrence and aperiodicity, hence it will eventually converge to the equilibrium distribution regardless of its initial state \cite{MCMC}. 
Since we are aiming to maximize the posterior likelihood, we need to calculate the likelihood after each iteration and record the configuration with the highest value.
The posterior likelihood is given by:
\begin{equation}
\begin{gathered}
p(\bm{c}|\bm{y}) \propto 
\left[\prod_{n_j \neq 0} \alpha \Gamma(n_j)\right]
\prod_{i=1}^n p(y_i|c_i, \bm{y}, \bm{c}_{-i})
\end{gathered}
\label{posterior_likelihood}
\end{equation}
Note that the probability in the second product doesn't have the $-i$ subscript on $\bm{y}$ like in Eq. (\ref{integral_1}), so the data doesn't need to be removed from its current cluster now.


We also need to define the base distribution $H$ that gives the prior distribution $p(\bm{\theta})$ and the concentration parameter $\alpha$ which controls the generation of new clusters.
Note that under the assumptions of DP, the final data distribution is a biased sample from $H$ unless $\alpha$ tends to infinity. Hence we don't need to ensure our $H$ being close to data distribution, while a rather conservative distribution is encouraged.
The $\alpha$ is selected based on Eq. (\ref{indicator_posterior_3}).
We can see that the probability of getting a new cluster is proportional to $\alpha$ and decreases asymptotically to zero as the number of data increases.
This is reasonable as the more data we have, the more certain we are that we have sampled at least one data from each cluster, hence the lesser we need a new cluster. 
If the $\alpha$ is infinite, DP believes all mixture component has the equal weight, and the data should always belong to a new situation regardless how many data we have collected.
In practice, we can estimate the amount of data we need in order to fully cover all situations. Then we can find the corresponding $\alpha$ using Eq. (\ref{indicator_posterior_3}) so that the probability decreases to a small value (e.g. 1\%) after we have collected this number of data.

DPMM is a very powerful tool to make non-parametric clustering without specifying the number of clusters. 
It can also easily incorporate the infinite limit than approaches working with finite models of unknown sizes like \cite{Bayesian_mixture}.
Comparing with optimization based methods like EM \cite{EM}, the use of MCMC can easily overcome the local optimal \cite{infinite_GMM}. 
For example, if there are duplicated mixture components, EM will still converge but the DPMM will eventually merge them as the Eq. (\ref{posterior_likelihood}) favours the data to be concentrated. 
Note that the above derivations of DPMM did not put any constrain on the type of distribution, namely the term $p(y|\bm{\theta})$ in Eq. (\ref{integral_1}). Such distribution can also be non-parametric distributions like Gaussian Processes. 
This means we can use DP to cluster the dynamics in RTE, which is GP, to build a mixture model of the real-world dynamics.


\input{methodology.tex}

%To formulate the problem clearly, we are aiming to leverage the collaboratively collected real-world interactions to assist a robot deployed in an unknown environment to quickly adapt and complete its task.
%We are provided with some episodes (each task-solving process corresponds to an episode of interaction data) of real-world interactions collected by several robots deployed in an unknown distribution of environments.
%The most intuitive idea is to label these historical data with their sources (like collected from grass terrain, collected with left leg broken, etc.) and build a simple database in the cloud server. 
%During the adaptation, the robot examines the current environment and uploads the result to the database server. The server then identifies the most similar environment recorded in the database and let the robot download the corresponding data to assist the its adaptation.
%However, accurately measuring and describing the environment can be expensive and difficult. It is also hard to determine whether a previously encountered environment shares similar dynamics with the current one even if we have accurate descriptions for them.
%In the case we do know that some historical data come from exactly the same environment as the current one, these data might be insufficient in quantity to assist the current task.
%Hence, it is reasonable to group the episodes resulting from similar dynamics into clusters, ensuring having enough data for analysis.
%We will assume that we donâ€™t have access to any information of the environment and aim to conduct clustering only according to the dynamics. 




\input{experiment.tex}










\section{Results and Analysis}

\subsection{Dirichlet Process Clustering}
The result of our clustering is presented in Fig \ref{clustering_results}.
We can see that the method managed to accurately group up the data from the same environment with only one miss classified episode in arc-based representation.
Further investigation shows this is a rather small episode containing only 9 interaction data. 
It is likely that when expressed in arc-based representation, the data in this episode happen to exhibit similar outcomes in both environments which finally led to this confusion.
However, another feature of the clustering result is that data from the same environment can spread in multiple clusters, and we even have many clusters containing only one episode.
There are two possible reasons for this phenomenon.
One is that some execution is very noisy, hence if an episode happens to include such interaction data, it might appear to be largely different from the rest of the episodes.
Another possible reason is that we applied posterior convergence too soon, and the weights calculated using Eq. (\ref{refitted_weights}) over-fits to the existing data, hence preventing further data from joining the cluster.
Fortunately, this should not affect the model prediction too much.


\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[width=0.22\textwidth]{xy_cluster_data.pdf}
%\label{a}
}
%\hfil
\subfloat[]{\includegraphics[width=0.22\textwidth]{arc_cluster_data.pdf}
%\label{b}
}
\caption{Cluster assignment of the data in Table \ref{data} after applying our DP clustering method.
(a) is the result using displacement-based representation, and (b) is the one for arc-based representation.}
\label{clustering_results}
\end{figure}


\begin{figure*}[!t]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{regular_case0.pdf}%
\label{case_0}}
%\hfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{regular_case1.pdf}%
\label{case_1}}
\vfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{regular_case2.pdf}%
\label{case_2}}
%\hfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{regular_case3.pdf}%
\label{case_3}}
\caption{The prediction error versus the step taken in each environment.
The error is defined to be the mean-squared distance between the predicted final position and the actual final position.
}
\label{dynamics_learning}
\end{figure*}

\subsection{Dynamics Learning}
Before deploying the robot in the maze, we first test the capability of our robot to quickly learn the dynamics using the infinite mixture of GP model.
We do this by placing our robot in a previously encountered environment and feeding the robot with a randomly selected interaction data in each step.
We then make the robot predict the outcomes for the entire repertoire and monitor the change of the prediction error.
This is to simulate the model-based online adaptation process where the robot executes a policy in each time-step and use the outcome to update its model.
To mitigate the randomness in the result, we repeat this 100 times and plotted out the mean and standard deviation in Fig \ref{dynamics_learning}.


We can see that for any previously encountered environment, our model greatly outperforms the RTE method.
Surprisingly, the displacement-based method is significantly better than the arc-based method.
The displacement-based method can typically reduce the error to below $1.0$ within 5 steps, while the arc-based method learns much slower and is lesser stable.
This could result from the fact that arc-based method uses three parameters unlike the use of two parameters in displacement-based representation.
It's likely that the extra dimension and the inconsistent measurement error in arc-based method induces more uncertainties than displacement-based approach and also make the selection of the prior lesser stable.
Nevertheless, both infinite mixture of GP methods are much better than RTE.
This suggests that it is a successful method to identify candidate priors from the historical data and try to figure out the most suitable prior with the data collected online.
We also see that the RTE starts up with a increase in error.
This is likely a sign that the use of $x$ and $y$ displacements as the input space fails to handle the case where two policies leading to similar displacements in the default simulation environment create distinct outcomes in a new environment.
This explains the rapid increase in the error as such input space could generate misleading result.
It is counter-intuitive to see that env 1 has the largest error while it is most similar to the default environment.
Further investigation finds it hard to achieve an error lesser than 0.6 in env 1.
It is also found that environments closer to the default one have larger uncertainties in policy execution.
This is likely because the effect of random perturbation will accumulate during the motion, and the faster the robot walks the greater the uncertainties will be.
The data size collected from each environment matters as the learning in env 4 (15 episodes) is the fastest, while that in env 3 (5 episodes) is slowest.

\begin{figure*}[!t]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{weight_2.pdf}%
\label{weight_2}}
%\hfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{weight_4.pdf}%
\label{weight_4}}
\caption{The change of the posterior weight of each cluster versus the number of steps taken.
These data correspond to the model using displacement-based dynamics representation.
}
\label{weights}
\end{figure*}

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{regular_case4.pdf}
\caption{Dynamics learning in an unseen environment.
For this environment, the robot damage is in the leg 4, the friction coefficient is 0.77 and the torque scale is 0.82}
\label{new_env}
\end{figure}

These online learnings are all made in previously encounter environments.
To examine the generalizability of our method, we also tested the adaptation on a new environment.
It can be seen from Fig \ref{new_env} that although this environment is new hence none of the historical data will help, our method still manges to adapt in this environment as good as RTE.
To monitor the selection of the prior during the adaptation, we plotted out the posterior weights of each cluster, namely the $p(c=j| \bm{x}_{1:N})$ in Eq. (\ref{predicted_mean}).
We see from Fig \ref{weight_2} that although the data for env 3 split into two clusters (see Fig \ref{clustering_results}), they collectively contribute to most of the posterior weights.
When deployed in the new environment, none of the existing clusters can explain the interaction data.
Thanks to the wonderful property of DP that the probability for getting a new cluster is never zero, we see from the posterior weights in Fig \ref{weight_4} that the robot eventually realizes that it is in a new situation and hence uses RTE to make prediction.


Finally, we tested our model's capability of learning new dynamics.
This is done by adding an episode of new data to our dataset and updating our model accordingly.
This episode is collected from the new environment in Fig \ref{new_env} and contains 35 data.
The result is presented in Fig \ref{new_adapt}.
We see that after incorporating this new episode into our dataset, the robot using displacement-based method quickly learns how to adapt in this new environment.
The arc-based method is also performing better, but it learns slower.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\textwidth]{new_case4.pdf}
\caption{Dynamics learning performance after having collected an episode of interactions from this environment.
With dataset updated, 20 extra Gibbs sampling sweeps were conducted to identify the best indicator configuration.}
\label{new_adapt}
\end{figure}


\begin{figure*}[!t]
\centering
\subfloat[]{\includegraphics[width=0.45\textwidth]{navigation_results_0.pdf}%
\label{maze_0}}
%\hfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{navigation_results_1.pdf}%
\label{maze_1}}
\vfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{navigation_results_2.pdf}%
\label{maze_2}}
%\hfil
\subfloat[]{\includegraphics[width=0.45\textwidth]{navigation_results_3.pdf}%
\label{maze_3}}
\caption{The effective distance travelled by the robot versus the step number.
Since the maze is U-shaped, we use effective distance instead of the distance to the goal to track the progress of the robot.
The effective distance is defined to be the projection on the suggested path, which is the distance on the path that is closet to the robot's current position.
The effective distance at the exit is 55.386.
}
\label{navigation_results}
\end{figure*}


\subsection{Maze Adaptation}


Since our method shows very promising result in dynamics learning, we hoped to
investigate the practical significance of this improvement in actual task solving.
We deployed our robot in each of the previously encountered environments and let it navigate through the maze.
The results are presented in Fig \ref{navigation_results}. 
We see that in all environments, the robots using our models completes the task significant faster than the one using RTE.
Although we have found that the displacement-based method provides much more accurate modelling of the dynamics than the arc-based method, the two robots showed little difference in performance except in env 2.
A very likely explanation for this is that for the policies that are used most often, the two methods provide similar accuracies.
By examining the plots, we see that most curves experience a decrease in its gradient at effective distance roughly equals to 20.
Referring to the floor-plan of the maze in Fig \ref{maze}, this effective distance corresponds to the first turning, after which the tunnel width is halved.
This poses a tougher requirement of the model predictions, which may otherwise lead to collisions.
We can see from Fig \ref{maze_0} and Fig \ref{maze_3} that the robot using RTE 
is not falling behind at start but fails to catch up after the first turning.
It is very likely that the model of the robot is not accurate enough for the narrowed tunnel and caused collisions during the motion.
We see from Fig \ref{maze_1} that the robot using arc-based method is likely suffering from similar issues in env 2.


To further analyse these results, the details of the maze adaptation are listed in Table \ref{detailed_data}.
We see that the robot using displacement-based method provides a very stunning improvement over RTE.
It takes around $35\%$ lesser steps to reach the goal except in env 1, where the dynamics is close to the baseline.
Similar improvement is offered by the arc-based method except for env 2.
By comparing the prediction error of the two methods, we see that for the policies that are executed during the adaptation, the two methods give very similar accuracies in their predictions except for env 2.
This is consistent with our speculation and explains the fact that the robot using arc-based method had $4.17$ more collisions in average when adapting in env 2.
However, it is strange that the arc-based method led to the most collisions in this environment, while RTE has much higher errors in its predictions.
A highly probably explanation for this is that while RTE provides much lesser accuracy, 


in many cases it chooses a policy that barely moves the robot but is also lesser likely to cause collisions.
 






\begin{table*}[t!]
\caption{Maze Adaptation Details}
\centering
%\def\arraystretch{1}
\begin{tabular}{c c c c c c c c c}
\hline
\addlinespace[0.1cm]
Method         & &       & DPMM xy-based & & DPMM arc-based & & RTE   & \\
\addlinespace[0.1cm]
\hline
\addlinespace[0.1cm]
               & & env 1 & $21.5 \pm 4.13$ & & $21.04 \pm 4.49$ & & $24.23 \pm 5.42$ & \\
Steps to reach & & env 2 & $23.52 \pm 3.36$ & & $32.21 \pm 8.01$ & & $37.61 \pm 8.75$ & \\
 the exit & & env 3 & $25.99 \pm 7.12$ & & $28.42 \pm 6.43$ & & $40.8 \pm 8.80$ & \\
               & & env 4 & $22.13 \pm 3.30$ & &  $22.93 \pm 3.78$ & & $33.38 \pm 4.86$   & \\
\addlinespace[0.1cm]
\hline
%
%
\addlinespace[0.1cm]
               & & env 1 & $3.64 \pm 1.99$ & & $3.35 \pm 2.07$ & & $5.06 \pm 2.49$ & \\
Number of      & & env 2 & $2.24 \pm 1.61$ & & $6.41 \pm 3.52$ & & $4.29 \pm 2.29$ & \\
collisions     & & env 3 & $2.72 \pm 2.01$ & & $4.6 \pm 2.46$ & & $3.73 \pm 2.81$ & \\
               & & env 4 & $1.31 \pm 1.49$ & & $2.07 \pm 1.76$ & & $2.91 \pm 1.91$   & \\
\addlinespace[0.1cm]
\hline
%
%
\addlinespace[0.1cm]
               & & env 1 & $57.459 \pm 1.70$ & & $57.262 \pm 1.85$ & & $57.447 \pm 1.91$ & \\
Total travelled& & env 2 & $57.669 \pm 2.05$ & & $58.077 \pm 2.60$ & & $59.833 \pm 3.08$ & \\
distance       & & env 3 & $57.754 \pm 2.02$ & & $57.546 \pm 1.88$ & & $60.921 \pm 2.46$ & \\
               & & env 4 & $56.822 \pm 1.60$ & & $ 57.683 \pm 1.59 $ & &  $58.4387 \pm 1.47$ & \\
\addlinespace[0.1cm]
\hline
%
%
\addlinespace[0.1cm]
               & & env 1 & $0.759 \pm 0.14$  & & $0.849 \pm 0.027$ & & $1.048 \pm 0.23$ & \\
Model prediction&& env 2 & $0.800 \pm 0.24$ & & $1.059 \pm 0.034$ & & $1.618 \pm 0.14$ & \\
error (RMSE)   & & env 3 & $1.122 \pm 0.27$  & & $1.162 \pm 0.024$ & & $1.479 \pm 0.22$ & \\
               & & env 4 & $0.709 \pm 0.17$ & & $0.678 \pm 0.021$ & & $1.346 \pm 0.14$ & \\
\addlinespace[0.1cm]
\hline
\end{tabular}
\label{detailed_data}
\end{table*}



\section{Conclusion}






%\begin{footnotesize}

\bibliographystyle{unsrt}
\bibliography{reference}
%\end{footnotesize}

%\end{multicols}
\end{document}