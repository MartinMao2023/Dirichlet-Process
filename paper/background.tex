
\section{Background}
\subsection{Quality-Diversity Optimization}
The repertoire-based method uses quality diversity (QD) optimization to generate a repertoire of diversified and high-quality locomotion behaviours. 
The most popular method for doing QD is called MAP-Elites \cite{QD, Map-Elites}, where we tabularize the task-space (for example, discretize the displacement of the robot along the x, y axis) into grids. 
Each grid corresponds to a policy that leads the robot to this part of task space. 
Initially the grids are empty, as we have no policies at start, and will be filled during a process of stochastic optimization inspired by natural evolutionary. 
We start from a few random policies, which are typically neural networks, and execute them in the simulator to get their outcomes in the task space. 
We also assign a performance score (called fitness) to each policy to quantify the efficiency of the execution. Typically, the fitness can be based on the energy consumption \cite{Q-Dax} or the shape of the trajectory \cite{evorbc_conf}.
Each of these outcomes is mapped to one of the girds (hence these grids are filled). 
We then begin our main loop of policy generation. 
In each iteration we select a few policies corresponding to the filled grids as the parents. We then conduct crossover on the parents to mutate their genotypes (for neural networks, these are just the weights of the neurons) to get their off-springs. 
The crossover operation should balance the exploitation and exploration by ensuring some inheritance of the parents to preserve good genes as well as some random variations for promoting diversity.
We then evaluate the off-springs in the simulator to get their outcomes in the task space and identify the grids they belong to.
If a grid is already occupied by another policy, we only keep the one with the higher fitness; if it is empty, we fill this grid with the corresponding off-spring. 
The pseudo code for MAP-Elites QD algorithm is presented below.

\begin{algorithm}
\caption{MAP-Elites}
\begin{algorithmic}
\STATE \textbf{procedure} MAP-Elites
\STATE Discretize the task space into grids
\STATE $\mathcal{R} \leftarrow \emptyset$ \COMMENT{Initialize repertoire}
\FOR{$\bm{i} = 1 \rightarrow G$}
\STATE $\theta \leftarrow $ generate{\_}random{\_}policy()
\STATE $\Theta$, $f$ $\leftarrow$ evaluate($\theta$) \COMMENT{Get outcome and fitness}
\STATE add{\_}to{\_}repertoire($\Theta$, $f$, $\theta$, $\mathcal{R}$) \COMMENT{Update repertoire}
\ENDFOR

\STATE \COMMENT{Main loop}
\FOR{$\bm{i} = 1 \rightarrow T$}
\STATE $\vartheta \leftarrow$ select{\_}parents($\mathcal{R}$)
\STATE $\bm{\theta} \leftarrow$ crossover($\vartheta$, $N$) \COMMENT{Generate $N$ off-springs}
\FOR{$\bm{j} = 1 \rightarrow N$}
\STATE $\Theta$, $f$ $\leftarrow$ evaluate($\bm{\theta_j}$)
\COMMENT{Off-spring evaluation}
\STATE add{\_}to{\_}repertoire($\Theta$, $f$, $\bm{\theta_j}$, $\mathcal{R}$)
\ENDFOR
\ENDFOR
\RETURN $\mathcal{R}$ \COMMENT{Output the final repertoire}
\end{algorithmic}
\label{MAP-Elites}
\end{algorithm}
\noindent
Evaluating the policies in the simulation is the major source of computationally expense for the MAP-Elites.
A merit of this algorithm is that the inner for loop of off-spring evaluation can be conducted in parallel, and the repertoire update can be made after having collected all the results of the off-springs.
This enables us to generate a large number of off-springs in each iteration and evaluate them in parallel, fully leveraging the power of modern Cluster computing \cite{Q-Dax}.
By running the algorithm for a few thousand iterations, we will end up having a repertoire of high-performing policies well-covering the task space.

\subsection{Gaussian Process}
The Gaussian Process (GP) is a powerful non-parametric machine learning algorithm. 
It assumes the prior distribution of the outcomes to be a multivariate normal distribution with a prior mean $\mu$ and a covariance matrix $\Sigma$. 
\begin{equation}
f(\bm{y}) = \frac{exp(
-\frac{1}{2}(\bm{y} - \bm{\mu})^T \bm{\Sigma^{-1}} (\bm{y} - \bm{\mu})
)}
{
\sqrt{(2\pi)^N \det(\bm{\Sigma})}
}
\label{multi_normal}
\end{equation}
In contrast to regular multivariate normal distribution, GP allows the prior mean and the covariance matrix to be input dependent. The
prior mean becomes a function $\mu(\bm{x})$, and the covariance between two data points is given by a covariance function 
$k(\bm{x_1}, \bm{x_2})$ called kernel. 
Following the fact that data points closer to each other are more correlated, this covariance starts from the prior variance and asymptotically decreases to zero as the distance between the two points tends to infinity. 
To account for the fact the observations might be noisy, we can incorporate the noise into the covariance matrix by adding the variance of the noise on the diagonal.
\begin{equation}
\bm{\Sigma} = \bm{K}(\bm{x_{1:N}}, \bm{x_{1:N}}) + \sigma_n^2 \mathbf{I}
\label{covariance_matrix}
\end{equation}

This allows us to build reasonable joint prior distribution for the values of any well-behaved functions.
To make prediction with GP, we first use the prior mean function and kernel to build up the joint distribution as the prior. 
Then we can calculate the posterior for the data we wish to evaluate conditioned on the ones we have observed using Bayes rule.
Since the Gaussian distribution is a conjugate distribution, the posterior is still a Gaussian with mean and variance given by \cite{GP, GP_posterior}:
\begin{equation}
\begin{gathered}
\mu_*(x) = \mu(x) + 
\bm{\Sigma}_{N, x}^T
\bm{\Sigma}_{N, N}^{-1}
(\bm{y}_{1:N} - \bm{\mu}(\bm{x}_{1:N}))
\\
\sigma^2_*(x) = \sigma^2(x) -  
\bm{\Sigma}_{N, x}^T
\bm{\Sigma}_{N, N}^{-1}
\bm{\Sigma}_{N, x}
\end{gathered}
\label{GP_posterior}
\end{equation}
where $\bm{\Sigma}_{N, N}$ denotes the covariance matrix of the observed data, and $\bm{\Sigma}_{N, x}$ denotes the column vector with entry on each row equal to the covariance between 
each observed data and the data we hope to evaluate.

Typically, the prior mean is just a fixed constant like zero, representing the prior knowledge we have about the outcome when there is no neighbouring data to refer to.
The typical choice of kernel can be RBF kernel that models the correlations to decrease in the form of a Gaussian function.
\begin{equation}
k(\bm{x_i}, \bm{x_j}) = \alpha_0 e^{-\frac{1}{2} d_{ij}^2}
\label{RBF_kernel}
\end{equation}
RBF kernel assumes the function is infinitely differentiable, which may be factually incorrect. Hence, it might be more desirable to use the Matern kernel:
\begin{equation}
k(\bm{x_i}, \bm{x_j}) = \alpha_0 \frac{2^{1-v}}{\Gamma(v)}
(\sqrt{2v} d_{ij})^v K_v(\sqrt{2v} d_{ij})
\label{Matern}
\end{equation}
where the $\Gamma$ is the gamma function, and the $K_v$ is the modified Bessel function of the second kind. The $v$ controls the smoothness, as GP using Matern kernel is $\lceil v \rceil - 1$ times differentiable in the mean-square sense \cite{GP, Matern}.
The distance $d_{ij}$ between two data points does not have to be Euclidean distance. A more general form would be:
\begin{equation}
d_{ij}^2 = \Sigma_{d=1}^D \frac{(x_{d, i} - x_{d, j})^2}{l_d^2}
\label{distance_metric}
\end{equation}
Where this distance metric can be anisotropic, suggesting distance along some axis contributes more uncertainty than others. 
The $l_d$ in the denominator is a positive number called length scale that determines how slowly we lose uncertainty (bigger length scale means the function is flatter) along the $d^\text{th}$ axis.


\subsection{Dirichlet Process Mixture Model}
Dirichlet process mixture model (DPMM), also called infinite mixture model, is a Bayesian non-parametric model widely used in data clustering. 
It is based on a stochastic process called Dirichlet Process (DP).
DP assumes that the data are sampled from a distribution of distributions. For example, the data points come from a set of Gaussian distributions, while the mean and variance of each Gaussian follow another distribution. 
To make clear explanation, we first investigate the finite mixture model using DP, and then extend the derivations to the infinite limit.
We know that finite mixture models can be represented as:
\begin{equation}
p(\bm{y}) = \Sigma_{j=1}^k \pi_j \cdot p(\bm{y}|\bm{\theta_j})
\label{finite_mixture_model}
\end{equation}
where this model consists of $k$ components, and $\bm{\theta_j}$ and $\pi_j$ are the parameters and the mixture weights (also called mixing proportions) for each mixture component.
DP assumes the parameters and the mixture weights are independently sampled. 
The parameters of the mixture components are sampled from a base distribution $H$, and the mixture weights are sampled from a symmetric Dirichlet distribution \cite{DP}:
 \begin{equation}
p(\pi_1, \dots, \pi_k | \alpha) = 
\frac{\Gamma(\alpha)}{\Gamma(\alpha / k)^k} \prod_{j=1}^k \pi_j^{a/k - 1}
\label{Dirichlet_distribution}
\end{equation}
Where the mixture weights are positive and sum up to 1. The constant $\alpha$ is called concentration parameter, which controls how dense the sampling will be.
To train a DPMM, the only thing we need to determine is the indicator of each data which points to the mixture component that this data belongs to. 
We then try to find the configuration of the indicators that maximizes the posterior likelihood (MAP estimate). 
This is very hard to do directly, typical alternative approaches include Gibbs sampling \cite{Gibbs_sampling, Gibbs} and the use of variational inference \cite{variational_method}.
This paper uses the Gibbs sampling method. 
In each iteration, we loop through each indicator and resample it based on the other indicators:
\begin{equation}
\begin{gathered}
p(c_i = j|y_i, \bm{y}_{-i}, \bm{c}_{-i}, \alpha) 
\propto  \\
p(c_i = j|\bm{c}_{-i}, \alpha) \cdot
p(y_i|c_i=j, \bm{y}_{-i}, \bm{c}_{-i})
\end{gathered}
\label{indicator_posterior}
\end{equation}
%
The subscript $-i$ means all the data except for $i$.
We know the probability of getting a certain configuration is:
\begin{equation}
p(c_1,\dots, c_n|\pi_1, \dots, \pi_k) = \prod_{j=1}^k \pi_j^{n_j}
\label{configuration_probability}
\end{equation}
Where the $n_j$ in the superscript denotes the number of data assigned to the $j^\text{th}$ component.
Using Eq. (\ref{Dirichlet_distribution}) and standard Dirichlet integral, we can calculate the probability density of such configuration in the prior:
\begin{equation}
\begin{gathered}
p(c_1, \dots, c_n | \alpha) =  \frac{\Gamma(\alpha)}
{\Gamma(\alpha / k)^k} \int \prod_{j=1}^k \pi_j^{n_j + a/k - 1} d\bm{\pi}
\\
= \frac{\Gamma(\alpha)}{\Gamma(\alpha + n)} 
\prod_{j=1}^k \frac{\Gamma(n_j + \alpha/k)}{\Gamma(\alpha/k)}
\end{gathered}
\label{indicator_prior}
\end{equation}
Hence we can find the posterior using Bayes rule:
\begin{equation}
p(c_i =j | \bm{c}_{-i} , \alpha) = 
\frac{n_{-i, j} + \alpha /k}{\alpha + n - 1}
\label{indicator_posterior_2}
\end{equation}
%
So far, we have been discussing the case of finite mixture models. If we let the component number $k$ tend to infinity, we will have the equations for infinite mixture models:
\begin{equation}
\begin{gathered}
p(c_i =j | \bm{c}_{-i} , \alpha) = 
\frac{n_{-i, j}}{\alpha + n - 1}
\\
p(c_i \neq j \, \text{for any} \, n_j \neq 0 | \bm{c}_{-i} , \alpha) = 
\frac{\alpha}{\alpha + n - 1}
\end{gathered}
\label{indicator_posterior_3}
\end{equation}
For any existing cluster (mixture component with at least one data assigned to it), this probability is proportional to the number of data in that cluster. 
Note that there is a non-zero probability that this data belongs to a new cluster. 
This is a very important property of DPMM that new clusters can be automatically generated based on the likelihood. 
To calculate the last term in Eq. (\ref{indicator_posterior}), we integrate the likelihood over the posterior:
\begin{equation}
\begin{gathered}
p(y_i|c_i, \bm{y}_{-i}, \bm{c}_{-i}) = 
\int p(y_i|\bm{\theta})
p(\bm{\theta}|c_i, \bm{y}_{-i}, \bm{c}_{-i})
d\bm{\theta}
%
\\
%
\text{where \,\,} 
p(\bm{\theta}|c_i, \bm{y}_{-i}, \bm{c}_{-i}) \propto
\,\, p(\bm{\theta})
\prod_{c_k=c_i, k \neq i} p(y_k|\bm{\theta})
\end{gathered}
\label{integral_1}
\end{equation}
In the case of a new cluster, the integration is made over the prior:
\begin{equation}
p(y_i|c_i \neq j \, \text{for any} \, n_j \neq 0) = 
\int p(y_i|\bm{\theta})
p(\bm{\theta})d\bm{\theta}
\label{integral_2}
\end{equation}
To conduct Gibbs sampling, we first start from an initial configuration of indicators (common choice is that every data is a cluster on its own).
Then in each iteration, we loop through each indicator and use Eq. (\ref{indicator_posterior}) to resample it. 
The $-i$ subscript on $\bm{y}$ means we need to remove the data from its current cluster during resampling, which will affect the result of Eq. (\ref{integral_1}) when calculating the probability of the data remaining in its current cluster. 
The Gibbs sampling is a Markov Chain Monte Carlo (MCMC) that satisfies irreducibility, positive recurrence and aperiodicity, hence it will eventually converge to the equilibrium distribution regardless of its initial state \cite{MCMC}. 
Since we are aiming to maximize the posterior likelihood, we need to calculate the likelihood after each iteration and record the configuration with the highest value.
The posterior likelihood is given by:
\begin{equation}
\begin{gathered}
p(\bm{c}|\bm{y}) \propto 
\left[\prod_{n_j \neq 0} \alpha \Gamma(n_j)\right]
\prod_{i=1}^n p(y_i|c_i, \bm{y}, \bm{c}_{-i})
\end{gathered}
\label{posterior_likelihood}
\end{equation}
Note that the probability in the second product doesn't have the $-i$ subscript on $\bm{y}$ like in Eq. (\ref{integral_1}), so the data doesn't need to be removed from its current cluster now.


We also need to define the base distribution $H$ that gives the prior distribution $p(\bm{\theta})$ and the concentration parameter $\alpha$ which controls the generation of new clusters.
Note that under the assumptions of DP, the final data distribution is a biased sample from $H$ unless $\alpha$ tends to infinity. Hence we don't need to ensure our $H$ being close to data distribution, while a rather conservative distribution is encouraged.
The $\alpha$ is selected based on Eq. (\ref{indicator_posterior_3}).
We can see that the probability of getting a new cluster is proportional to $\alpha$ and decreases asymptotically to zero as the number of data increases.
This is reasonable as the more data we have, the more certain we are that we have sampled at least one data from each cluster, hence the lesser we need a new cluster. 
If the $\alpha$ is infinite, DP believes all mixture component has the equal weight, and the data should always belong to a new situation regardless how many data we have collected.
In practice, we can estimate the amount of data we need in order to fully cover all situations. Then we can find the corresponding $\alpha$ using Eq. (\ref{indicator_posterior_3}) so that the probability decreases to a small value (e.g. 1\%) after we have collected this number of data.

DPMM is a very powerful tool to make non-parametric clustering without specifying the number of clusters. 
It can also easily incorporate the infinite limit than approaches working with finite models of unknown sizes like \cite{Bayesian_mixture}.
Comparing with optimization based methods like EM \cite{EM}, the use of MCMC can easily overcome the local optimal \cite{infinite_GMM}. 
For example, if there are duplicated mixture components, EM will still converge but the DPMM will eventually merge them as the Eq. (\ref{posterior_likelihood}) favours the data to be concentrated. 
Note that the above derivations of DPMM did not put any constrain on the type of distribution, namely the term $p(y|\bm{\theta})$ in Eq. (\ref{integral_1}). Such distribution can also be non-parametric distributions like Gaussian Processes. 
