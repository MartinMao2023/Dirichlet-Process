{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lg\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from Matern import Matern\n",
    "from Adaptor import Adaptor_xy\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archive(metaclass=ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_priors(self, paths: dict):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_experiences(self, exps):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def gibbs_sweep(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def MAP(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, dir_path=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, dir_path):\n",
    "        pass\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archive_xy(Archive):\n",
    "    def __init__(self, alpha=1.0, threshold=40):\n",
    "        self.alpha = alpha\n",
    "        self.kernel = Matern(v=1.5)\n",
    "        self.exps = {}\n",
    "        self.next_exp_id = 1\n",
    "        self.next_cluster_id = 1\n",
    "        self.empty_cluster_ids = []\n",
    "        self.empty_exp_ids = []\n",
    "        self.clusters = {}\n",
    "        self.threshold = threshold\n",
    "        self.backup_clusters = {}\n",
    "        self.x_priors = None\n",
    "        self.y_priors = None\n",
    "        self.log_prob = -np.inf\n",
    "\n",
    "    \n",
    "    def load_priors(self, paths):\n",
    "        x_priors = np.load(paths[\"x\"])\n",
    "        self.x_prior_data = x_priors.copy()\n",
    "        y_priors = np.load(paths[\"y\"])\n",
    "        self.y_prior_data = y_priors.copy()\n",
    "\n",
    "        self.x_priors = []\n",
    "        for x_prior in x_priors:\n",
    "            coef = x_prior[:5].reshape(-1, 1)\n",
    "            length_scale = x_prior[5:10]\n",
    "            prior_var, noise_ratio = x_prior[10:12]\n",
    "            noisy = True if x_prior[-1] else False\n",
    "            self.x_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))\n",
    "        \n",
    "        self.y_priors = []\n",
    "        for y_prior in y_priors:\n",
    "            coef = y_prior[:5].reshape(-1, 1)\n",
    "            length_scale = y_prior[5:10]\n",
    "            prior_var, noise_ratio = y_prior[10:12]\n",
    "            noisy = True if y_prior[-1] else False\n",
    "            self.y_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))\n",
    "    \n",
    "\n",
    "    def read_experiences(self, exps, ids=None):\n",
    "        if ids is None and exps:\n",
    "            self.log_prob = -np.inf\n",
    "\n",
    "        for id_index, exp in enumerate(exps):\n",
    "            exp_dict = {}\n",
    "            x_data = exp[\"x\"]\n",
    "            y_data = exp[\"y\"]\n",
    "            exp_dict[\"data_size\"] = len(x_data)\n",
    "            exp_dict[\"x_data\"] = x_data.copy()\n",
    "            exp_dict[\"y_data\"] = y_data.copy()\n",
    "\n",
    "            if not ids is None:\n",
    "                exp_id = ids[id_index]\n",
    "            elif self.empty_exp_ids:\n",
    "                exp_id = self.empty_exp_ids[0]\n",
    "                del self.empty_exp_ids[0]\n",
    "            else:\n",
    "                exp_id = self.next_exp_id\n",
    "                self.next_exp_id += 1\n",
    "\n",
    "            exp_dict[\"id\"] = exp_id\n",
    "            self.exps[exp_id] = exp_dict\n",
    "            x_loglikelihoods = np.zeros(len(self.x_priors), dtype=np.float32)\n",
    "            y_loglikelihoods = np.zeros(len(self.y_priors), dtype=np.float32)\n",
    "\n",
    "            x_coors = x_data[:, :-1]\n",
    "            x_target = x_data[:, -1].reshape(-1, 1)\n",
    "            y_coors = y_data[:, :-1]\n",
    "            y_target = y_data[:, -1].reshape(-1, 1)\n",
    "            \n",
    "            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.x_priors):\n",
    "                diff = x_target - x_coors @ coef\n",
    "                data_size = len(x_data)\n",
    "                if noisy:\n",
    "                    x_loglikelihood = -0.5*(diff / (prior_var*noise_ratio + prior_var)).T @ diff - \\\n",
    "                        0.5*data_size*math.log(prior_var*noise_ratio + prior_var)\n",
    "                else:\n",
    "                    cov = self.kernel(x_coors, length_scales=length_scale) + np.eye(data_size)*noise_ratio\n",
    "                    cov *= prior_var\n",
    "                    x_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)\n",
    "                    sign, log_det = lg.slogdet(cov)\n",
    "                    log_det = log_det if sign else -100\n",
    "                    x_loglikelihood -= 0.5*log_det\n",
    "                x_loglikelihoods[index] = float(x_loglikelihood)\n",
    "\n",
    "            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.y_priors):\n",
    "                diff = y_target - y_coors @ coef\n",
    "                data_size = len(y_data)\n",
    "                if noisy:\n",
    "                    y_loglikelihood = -0.5*(diff / (prior_var*noise_ratio + prior_var)).T @ diff - \\\n",
    "                        0.5*data_size*math.log(prior_var*noise_ratio + prior_var)\n",
    "                else:\n",
    "                    cov = self.kernel(y_coors, length_scales=length_scale) + np.eye(data_size)*noise_ratio\n",
    "                    cov *= prior_var\n",
    "                    y_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)\n",
    "                    sign, log_det = lg.slogdet(cov)\n",
    "                    log_det = log_det if sign else -100\n",
    "                    y_loglikelihood -= 0.5*log_det\n",
    "                y_loglikelihoods[index] = float(y_loglikelihood)\n",
    "            exp_dict[\"x_loglikelihoods\"] = x_loglikelihoods\n",
    "            exp_dict[\"y_loglikelihoods\"] = y_loglikelihoods\n",
    "            log_self_prob = np.log(np.mean(\n",
    "                np.exp(x_loglikelihoods - np.max(x_loglikelihoods))\n",
    "                )*np.mean(np.exp(y_loglikelihoods - np.max(y_loglikelihoods))))\n",
    "            log_self_prob += np.max(x_loglikelihoods) + np.max(y_loglikelihoods)\n",
    "            exp_dict[\"log_self_prob\"] = log_self_prob\n",
    "\n",
    "            if ids is None:\n",
    "                exp_dict[\"cluster\"] = self.alloc_cluster(exp_id)\n",
    "            else:\n",
    "                exp_dict[\"cluster\"] = 0\n",
    "        \n",
    "    \n",
    "\n",
    "    def drop_exp(self, exp_id):\n",
    "        exp = self.exps[exp_id]\n",
    "        self.remove_exp_from_cluster(exp_id, exp[\"cluster\"])\n",
    "        del self.exps[exp_id]\n",
    "        self.empty_exp_ids.append(exp_id)\n",
    "        self.log_prob = -np.inf\n",
    "\n",
    "\n",
    "\n",
    "    def alloc_cluster(self, exp_id):\n",
    "        if self.empty_cluster_ids:\n",
    "            cluster_id = self.empty_cluster_ids[0]\n",
    "            del self.empty_cluster_ids[0]\n",
    "        else:\n",
    "            cluster_id = self.next_cluster_id\n",
    "            self.next_cluster_id += 1\n",
    "        \n",
    "        cluster_dict = {\"id\": cluster_id}\n",
    "        cluster_dict[\"members\"] = {exp_id}\n",
    "        x_loglikelihoods = self.exps[exp_id][\"x_loglikelihoods\"].copy()\n",
    "        y_loglikelihoods = self.exps[exp_id][\"y_loglikelihoods\"].copy()\n",
    "        cluster_dict[\"x_loglikelihoods\"] = x_loglikelihoods\n",
    "        cluster_dict[\"y_loglikelihoods\"] = y_loglikelihoods\n",
    "        x_weights = np.exp(x_loglikelihoods - np.max(x_loglikelihoods))\n",
    "        y_weights = np.exp(y_loglikelihoods - np.max(y_loglikelihoods))\n",
    "        cluster_dict[\"x_weights\"] = x_weights / np.sum(x_weights)\n",
    "        cluster_dict[\"y_weights\"] = y_weights / np.sum(y_weights)\n",
    "        cluster_dict[\"size\"] = 1\n",
    "        cluster_dict[\"data_size\"] = self.exps[exp_id][\"data_size\"]\n",
    "        self.clusters[cluster_id] = cluster_dict\n",
    "        if cluster_dict[\"data_size\"] >= self.threshold:\n",
    "            cluster_dict[\"large\"] = True\n",
    "            cluster_dict[\"x_kernel_index\"] = int(np.argmax(x_weights))\n",
    "            cluster_dict[\"y_kernel_index\"] = int(np.argmax(y_weights))\n",
    "            x_coef, y_coef = self.fit_coef(cluster_id)\n",
    "            cluster_dict[\"x_coef\"] = x_coef\n",
    "            cluster_dict[\"y_coef\"] = y_coef\n",
    "        else:\n",
    "            cluster_dict[\"large\"] = False\n",
    "            cluster_dict[\"x_coef\"] = np.ones((5, 1), dtype=np.float32)\n",
    "            cluster_dict[\"y_coef\"] = np.ones((5, 1), dtype=np.float32)\n",
    "            cluster_dict[\"x_kernel_index\"] = 0\n",
    "            cluster_dict[\"y_kernel_index\"] = 0\n",
    "\n",
    "        return cluster_id\n",
    "\n",
    "    \n",
    "\n",
    "    def add_exp_to_cluster(self, exp_id, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        cluster[\"members\"].add(exp_id)\n",
    "        self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "        x_loglikelihoods = self.exps[exp_id][\"x_loglikelihoods\"]\n",
    "        y_loglikelihoods = self.exps[exp_id][\"y_loglikelihoods\"]\n",
    "        cluster[\"x_loglikelihoods\"] += x_loglikelihoods\n",
    "        cluster[\"y_loglikelihoods\"] += y_loglikelihoods\n",
    "        new_x_loglikelihoods = cluster[\"x_loglikelihoods\"]\n",
    "        new_y_loglikelihoods = cluster[\"y_loglikelihoods\"]\n",
    "        x_weights = np.exp(new_x_loglikelihoods - np.max(new_x_loglikelihoods))\n",
    "        y_weights = np.exp(new_y_loglikelihoods - np.max(new_y_loglikelihoods))\n",
    "        cluster[\"x_weights\"] = x_weights / np.sum(x_weights)\n",
    "        cluster[\"y_weights\"] = y_weights / np.sum(y_weights)\n",
    "        cluster[\"size\"] += 1\n",
    "        cluster[\"data_size\"] += self.exps[exp_id][\"data_size\"]\n",
    "\n",
    "        if cluster[\"data_size\"] >= self.threshold:\n",
    "            cluster[\"large\"] = False\n",
    "            cluster[\"x_kernel_index\"] = int(np.argmax(x_weights))\n",
    "            cluster[\"y_kernel_index\"] = int(np.argmax(y_weights))\n",
    "            x_coef, y_coef = self.fit_coef(cluster_id)\n",
    "            cluster[\"x_coef\"] = x_coef\n",
    "            cluster[\"y_coef\"] = y_coef\n",
    "\n",
    "\n",
    "\n",
    "    def remove_exp_from_cluster(self, exp_id, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        if cluster[\"size\"] == 1:\n",
    "            self.empty_cluster_ids.append(cluster_id)\n",
    "            del self.clusters[cluster_id]\n",
    "            return\n",
    "\n",
    "        cluster[\"members\"].discard(exp_id)\n",
    "        x_loglikelihoods = self.exps[exp_id][\"x_loglikelihoods\"]\n",
    "        y_loglikelihoods = self.exps[exp_id][\"y_loglikelihoods\"]\n",
    "        cluster[\"x_loglikelihoods\"] -= x_loglikelihoods\n",
    "        cluster[\"y_loglikelihoods\"] -= y_loglikelihoods\n",
    "        new_x_loglikelihoods = cluster[\"x_loglikelihoods\"]\n",
    "        new_y_loglikelihoods = cluster[\"y_loglikelihoods\"]\n",
    "        x_weights = np.exp(new_x_loglikelihoods - np.max(new_x_loglikelihoods))\n",
    "        y_weights = np.exp(new_y_loglikelihoods - np.max(new_y_loglikelihoods))\n",
    "        cluster[\"x_weights\"] = x_weights / np.sum(x_weights)\n",
    "        cluster[\"y_weights\"] = y_weights / np.sum(y_weights)\n",
    "        cluster[\"size\"] -= 1\n",
    "        cluster[\"data_size\"] -= self.exps[exp_id][\"data_size\"]\n",
    "\n",
    "        if cluster[\"data_size\"] >= self.threshold:\n",
    "            cluster[\"x_kernel_index\"] = int(np.argmax(x_weights))\n",
    "            cluster[\"y_kernel_index\"] = int(np.argmax(y_weights))\n",
    "            x_coef, y_coef = self.fit_coef(cluster_id)\n",
    "            cluster[\"x_coef\"] = x_coef\n",
    "            cluster[\"y_coef\"] = y_coef\n",
    "        else:\n",
    "            cluster[\"large\"] = False\n",
    "\n",
    "\n",
    "    \n",
    "    def fit_coef(self, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        x_noisy, _, x_length_scale, x_prior_var, x_noise_ratio = self.x_priors[cluster[\"x_kernel_index\"]]\n",
    "        y_noisy, _, y_length_scale, y_prior_var, y_noise_ratio = self.y_priors[cluster[\"y_kernel_index\"]]\n",
    "        x_coors = [self.exps[exp_id][\"x_data\"][:, :-1] for exp_id in cluster[\"members\"]]\n",
    "        x_targets = [self.exps[exp_id][\"x_data\"][:, -1].reshape(-1, 1) for exp_id in cluster[\"members\"]]\n",
    "        y_coors = [self.exps[exp_id][\"y_data\"][:, :-1] for exp_id in cluster[\"members\"]]\n",
    "        y_targets = [self.exps[exp_id][\"y_data\"][:, -1].reshape(-1, 1) for exp_id in cluster[\"members\"]]\n",
    "\n",
    "        if x_noisy:\n",
    "            A = sum([x_coor.T @ x_coor for x_coor in x_coors])\n",
    "            B = sum([x_coor.T @ x_target for x_coor, x_target in zip(x_coors, x_targets)])\n",
    "            x_coef = lg.inv(A) @ B\n",
    "        else:\n",
    "            inverse_covs = [lg.inv(\n",
    "                (self.kernel(x_coor, length_scales=x_length_scale) + np.eye(len(x_coor))*x_noise_ratio)*x_prior_var\n",
    "                ) for x_coor in x_coors]\n",
    "            A = sum([x_coor.T @ inverse_cov @ x_coor for x_coor, inverse_cov in zip(x_coors, inverse_covs)])\n",
    "            B = sum([x_coor.T @ inverse_cov @ x_target for x_coor, x_target, inverse_cov in zip(x_coors, x_targets, inverse_covs)])\n",
    "            x_coef = lg.inv(A) @ B\n",
    "\n",
    "        if y_noisy:\n",
    "            A = sum([y_coor.T @ y_coor for y_coor in y_coors])\n",
    "            B = sum([y_coor.T @ y_target for y_coor, y_target in zip(y_coors, y_targets)])\n",
    "            y_coef = lg.inv(A) @ B\n",
    "        else:\n",
    "            inverse_covs = [lg.inv(\n",
    "                (self.kernel(y_coor, length_scales=y_length_scale) + np.eye(len(y_coor))*y_noise_ratio)*y_prior_var\n",
    "                ) for y_coor in y_coors]\n",
    "            A = sum([y_coor.T @ inverse_cov @ y_coor for y_coor, inverse_cov in zip(y_coors, inverse_covs)])\n",
    "            B = sum([y_coor.T @ inverse_cov @ y_target for y_coor, y_target, inverse_cov in zip(y_coors, y_targets, inverse_covs)])\n",
    "            y_coef = lg.inv(A) @ B\n",
    "\n",
    "        return x_coef, y_coef\n",
    "\n",
    "\n",
    "\n",
    "    def gibbs_sweep(self, return_log_prob=False):\n",
    "        for exp_id in self.exps:\n",
    "            exp = self.exps[exp_id]\n",
    "            log_probs = np.zeros(len(self.clusters)+1, dtype=np.float32)\n",
    "            clusters = np.zeros(len(self.clusters)+1, dtype=np.int32)\n",
    "            log_probs[0] = exp[\"log_self_prob\"] + math.log(self.alpha)\n",
    "            for index, cluster_id in enumerate(self.clusters, 1):\n",
    "                solo_cluster, log_prob = self.calculate_likelihood(exp_id, cluster_id)\n",
    "                if solo_cluster:\n",
    "                    log_probs[index] = log_probs[0]\n",
    "                    log_probs[0] = -np.inf\n",
    "                elif exp[\"cluster\"] == cluster_id:\n",
    "                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id][\"size\"] - 1)\n",
    "                else:\n",
    "                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id][\"size\"])\n",
    "\n",
    "                clusters[index] = cluster_id\n",
    "            \n",
    "            log_probs -= np.max(log_probs)\n",
    "            probs = np.exp(log_probs)\n",
    "            probs /= np.sum(probs)\n",
    "            selected_cluster = np.random.choice(clusters, p=probs)\n",
    "            \n",
    "            original_cluster_id = exp[\"cluster\"]\n",
    "            if selected_cluster != original_cluster_id:\n",
    "                self.remove_exp_from_cluster(exp_id, original_cluster_id)\n",
    "                if selected_cluster:\n",
    "                    self.add_exp_to_cluster(exp_id, int(selected_cluster))\n",
    "                else:\n",
    "                    exp[\"cluster\"] = self.alloc_cluster(exp_id)\n",
    "        \n",
    "        if not return_log_prob:\n",
    "            return \n",
    "        \n",
    "        log_prob = 0\n",
    "        for cluster in self.clusters.values():\n",
    "            log_prob += math.lgamma(cluster[\"size\"]) + math.log(self.alpha)\n",
    "            if cluster[\"large\"]:\n",
    "                x_noisy, _, x_length_scale, x_prior_var, x_noise_ratio = self.x_priors[cluster[\"x_kernel_index\"]]\n",
    "                y_noisy, _, y_length_scale, y_prior_var, y_noise_ratio = self.y_priors[cluster[\"y_kernel_index\"]]\n",
    "                x_coef = cluster[\"x_coef\"]\n",
    "                y_coef = cluster[\"y_coef\"]\n",
    "                for exp_id in cluster[\"members\"]:\n",
    "\n",
    "                    x_coor = self.exps[exp_id][\"x_data\"][:, :-1]\n",
    "                    x_target = self.exps[exp_id][\"x_data\"][:, -1].reshape(-1, 1)\n",
    "                    y_coor = self.exps[exp_id][\"y_data\"][:, :-1]\n",
    "                    y_target = self.exps[exp_id][\"y_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "                    x_diff = x_target - x_coor @ x_coef\n",
    "                    y_diff = y_target - y_coor @ y_coef\n",
    "                    data_size = self.exps[exp_id][\"data_size\"]\n",
    "\n",
    "                    if x_noisy:\n",
    "                        x_loglikelihood = -0.5*(x_diff / (x_prior_var*x_noise_ratio + x_prior_var)).T @ x_diff - \\\n",
    "                            0.5*data_size*math.log(x_prior_var*x_noise_ratio + x_prior_var)\n",
    "                    else:\n",
    "                        cov = x_prior_var*self.kernel(x_coor, length_scales=x_length_scale) + np.eye(data_size)*x_noise_ratio*x_prior_var\n",
    "                        x_loglikelihood = -0.5*(x_diff.T @ lg.inv(cov) @ x_diff)\n",
    "                        sign, log_det = lg.slogdet(cov)\n",
    "                        log_det = log_det if sign else -100\n",
    "                        x_loglikelihood -= 0.5*log_det\n",
    "\n",
    "                    if y_noisy:\n",
    "                        y_loglikelihood = -0.5*(y_diff / (y_prior_var*y_noise_ratio + y_prior_var)).T @ y_diff - \\\n",
    "                            0.5*data_size*math.log(y_prior_var*y_noise_ratio + y_prior_var)\n",
    "                    else:\n",
    "                        cov = y_prior_var*self.kernel(y_coor, length_scales=y_length_scale) + np.eye(data_size)*y_noise_ratio*y_prior_var\n",
    "                        y_loglikelihood = -0.5*(y_diff.T @ lg.inv(cov) @ y_diff)\n",
    "                        sign, log_det = lg.slogdet(cov)\n",
    "                        log_det = log_det if sign else -100\n",
    "                        y_loglikelihood -= 0.5*log_det\n",
    "\n",
    "                    log_prob += float(x_loglikelihood + y_loglikelihood)\n",
    "            else:\n",
    "                x_log_likelihoods = cluster[\"x_loglikelihoods\"]\n",
    "                x_weights = cluster[\"x_weights\"]\n",
    "                log_prob += np.max(x_log_likelihoods)\n",
    "                log_prob += np.log(np.sum(x_weights * np.exp(x_log_likelihoods - np.max(x_log_likelihoods))))\n",
    "\n",
    "                y_log_likelihoods = cluster[\"y_loglikelihoods\"]\n",
    "                y_weights = cluster[\"y_weights\"]\n",
    "                log_prob += np.max(y_log_likelihoods)\n",
    "                log_prob += np.log(np.sum(y_weights * np.exp(y_log_likelihoods - np.max(y_log_likelihoods))))\n",
    "\n",
    "        return np.float32(log_prob)\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, exp_id, cluster_id):\n",
    "        exp = self.exps[exp_id]\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        solo_cluster = False\n",
    "\n",
    "        if exp[\"cluster\"] == cluster_id and cluster[\"size\"] == 1:\n",
    "            solo_cluster = True\n",
    "            log_prob = 0\n",
    "        elif exp[\"cluster\"] == cluster_id and (cluster[\"data_size\"] - exp[\"data_size\"]) < self.threshold:\n",
    "            x_loglikelihoods = cluster[\"x_loglikelihoods\"] - exp[\"x_loglikelihoods\"]\n",
    "            y_loglikelihoods = cluster[\"y_loglikelihoods\"] - exp[\"y_loglikelihoods\"]\n",
    "\n",
    "            x_weights = np.exp(x_loglikelihoods - np.max(x_loglikelihoods))\n",
    "            y_weights = np.exp(y_loglikelihoods - np.max(y_loglikelihoods))\n",
    "            x_weights /= np.sum(x_weights)\n",
    "            y_weights /= np.sum(y_weights)\n",
    "\n",
    "            x_log_scale = np.max(exp[\"x_loglikelihoods\"])\n",
    "            y_log_scale = np.max(exp[\"y_loglikelihoods\"])\n",
    "            x_prob = np.sum(x_weights * np.exp(exp[\"x_loglikelihoods\"] - x_log_scale))\n",
    "            y_prob = np.sum(y_weights * np.exp(exp[\"y_loglikelihoods\"] - y_log_scale))\n",
    "            prob = x_prob * y_prob\n",
    "            log_prob = x_log_scale + y_log_scale + (math.log(prob) if prob else -40)\n",
    "\n",
    "        elif not cluster[\"large\"]:\n",
    "            x_log_scale = np.max(exp[\"x_loglikelihoods\"])\n",
    "            y_log_scale = np.max(exp[\"y_loglikelihoods\"])\n",
    "            x_prob = np.sum(cluster[\"x_weights\"] * np.exp(exp[\"x_loglikelihoods\"] - x_log_scale))\n",
    "            y_prob = np.sum(cluster[\"y_weights\"] * np.exp(exp[\"y_loglikelihoods\"] - y_log_scale))\n",
    "            prob = x_prob * y_prob\n",
    "            log_prob = x_log_scale + y_log_scale + (math.log(prob) if prob else -40)\n",
    "        else:\n",
    "            x_noisy, _, x_length_scale, x_prior_var, x_noise_ratio = self.x_priors[cluster[\"x_kernel_index\"]]\n",
    "            y_noisy, _, y_length_scale, y_prior_var, y_noise_ratio = self.y_priors[cluster[\"y_kernel_index\"]]\n",
    "\n",
    "            x_coor = self.exps[exp_id][\"x_data\"][:, :-1]\n",
    "            x_target = self.exps[exp_id][\"x_data\"][:, -1].reshape(-1, 1)\n",
    "            y_coor = self.exps[exp_id][\"y_data\"][:, :-1]\n",
    "            y_target = self.exps[exp_id][\"y_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "            x_diff = x_target - x_coor @ cluster[\"x_coef\"]\n",
    "            y_diff = y_target - y_coor @ cluster[\"y_coef\"]\n",
    "            data_size = exp[\"data_size\"]\n",
    "\n",
    "            if x_noisy:\n",
    "                x_loglikelihood = -0.5*(x_diff / (x_prior_var*x_noise_ratio + x_prior_var)).T @ x_diff - \\\n",
    "                    0.5*data_size*math.log(x_prior_var*x_noise_ratio + x_prior_var)\n",
    "            else:\n",
    "                cov = x_prior_var*self.kernel(x_coor, length_scales=x_length_scale) + np.eye(data_size)*x_noise_ratio*x_prior_var\n",
    "                x_loglikelihood = -0.5*(x_diff.T @ lg.inv(cov) @ x_diff)\n",
    "                sign, log_det = lg.slogdet(cov)\n",
    "                log_det = log_det if sign else -100\n",
    "                x_loglikelihood -= 0.5*log_det\n",
    "\n",
    "            if y_noisy:\n",
    "                y_loglikelihood = -0.5*(y_diff / (y_prior_var*y_noise_ratio + y_prior_var)).T @ y_diff - \\\n",
    "                    0.5*data_size*math.log(y_prior_var*y_noise_ratio + y_prior_var)\n",
    "            else:\n",
    "                cov = y_prior_var*self.kernel(y_coor, length_scales=y_length_scale) + np.eye(data_size)*y_noise_ratio*y_prior_var\n",
    "                y_loglikelihood = -0.5*(y_diff.T @ lg.inv(cov) @ y_diff)\n",
    "                sign, log_det = lg.slogdet(cov)\n",
    "                log_det = log_det if sign else -100\n",
    "                y_loglikelihood -= 0.5*log_det\n",
    "\n",
    "            log_prob = x_loglikelihood + y_loglikelihood\n",
    "\n",
    "        return solo_cluster, log_prob\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.backup_clusters = {int(key): {i: copy.deepcopy(j) for i, j in value.items()} for key, value in self.clusters.items()}\n",
    "        self.backup_next_cluster_id = np.int32(self.next_cluster_id)\n",
    "        self.backup_empty_cluster_ids = self.empty_cluster_ids.copy()\n",
    "    \n",
    "\n",
    "\n",
    "    def MAP(self, rounds=20):\n",
    "        for r in range(rounds):\n",
    "            new_log_prob = self.gibbs_sweep(return_log_prob=True)\n",
    "            if new_log_prob > self.log_prob:\n",
    "                self.freeze()\n",
    "                self.log_prob = new_log_prob\n",
    "        \n",
    "        self.clusters = {int(key): {i: copy.deepcopy(j) for i, j in value.items()} for key, value in self.backup_clusters.items()}\n",
    "        self.next_cluster_id = int(self.backup_next_cluster_id)\n",
    "        self.empty_cluster_ids = [i for i in self.backup_empty_cluster_ids]\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            for exp_id in cluster[\"members\"]:\n",
    "                self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, dir_path=None):\n",
    "        dir_path = \"archive_xy\" if dir_path is None else dir_path\n",
    "        base_configs = {\"alpha\": self.alpha, \n",
    "                        \"next_exp_id\": self.next_exp_id, \n",
    "                        \"next_cluster_id\": self.next_cluster_id, \n",
    "                        \"empty_cluster_ids\": self.empty_cluster_ids.copy(), \n",
    "                        \"empty_exp_ids\": self.empty_exp_ids.copy(), \n",
    "                        \"threshold\": self.threshold} # <--\n",
    "        \n",
    "        exp_seps = [] \n",
    "        exp_length = 0\n",
    "        exps_x_data = []\n",
    "        exps_y_data = []\n",
    "\n",
    "        for exp_id, exp in self.exps.items():\n",
    "            exp_seps.append((exp_id, exp_length, exp_length + exp[\"data_size\"]))\n",
    "            exp_length += exp[\"data_size\"]\n",
    "            exps_x_data.append(exp[\"x_data\"])\n",
    "            exps_y_data.append(exp[\"y_data\"])\n",
    "        base_configs[\"exp_seps\"] = exp_seps\n",
    "        \n",
    "        exps_x_data = np.concatenate(exps_x_data, axis=0)[None, :, :]\n",
    "        exps_y_data = np.concatenate(exps_y_data, axis=0)[None, :, :]\n",
    "        exp_data = np.concatenate((exps_x_data, exps_y_data), axis=0) # <--\n",
    "\n",
    "        clusters_data = {} # <--\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            cluster_info = {}\n",
    "            for key, value in cluster.items():\n",
    "                if type(value) == np.ndarray:\n",
    "                    cluster_info[key] = value.tolist()\n",
    "                elif type(value) == set:\n",
    "                    cluster_info[key] = list(value)\n",
    "                else:\n",
    "                    cluster_info[key] = value\n",
    "            \n",
    "            clusters_data[cluster_id] = cluster_info\n",
    "        \n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "        np.save(os.path.join(dir_path, \"exp_data.npy\"), exp_data)\n",
    "        np.save(os.path.join(dir_path, \"GP_x.npy\"), self.x_prior_data)\n",
    "        np.save(os.path.join(dir_path, \"GP_y.npy\"), self.y_prior_data)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"base_configs.json\"), \"w\") as file:\n",
    "            json.dump(base_configs, file)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"cluster_data.json\"), \"w\") as file:\n",
    "            json.dump(clusters_data, file)\n",
    "        \n",
    "\n",
    "\n",
    "    def load(self, dir_path):\n",
    "        prior_path = {\"x\": os.path.join(dir_path, \"GP_x.npy\"), \n",
    "                      \"y\": os.path.join(dir_path, \"GP_y.npy\")}\n",
    "        self.load_priors(prior_path)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"base_configs.json\"), \"r\") as file:\n",
    "            base_configs = json.load(file)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"cluster_data.json\"), \"r\") as file:\n",
    "            cluster_data = json.load(file)\n",
    "        \n",
    "        exp_data = np.load(os.path.join(dir_path, \"exp_data.npy\"))\n",
    "\n",
    "        self.alpha = base_configs[\"alpha\"]\n",
    "        self.exps = {}\n",
    "        self.clusters = {}\n",
    "        self.next_exp_id = base_configs[\"next_exp_id\"]\n",
    "        self.next_cluster_id = base_configs[\"next_cluster_id\"]\n",
    "        self.empty_cluster_ids = base_configs[\"empty_cluster_ids\"]\n",
    "        self.empty_exp_ids = base_configs[\"empty_exp_ids\"]\n",
    "        self.threshold = base_configs[\"threshold\"]\n",
    "\n",
    "        exp_seps = base_configs[\"exp_seps\"]\n",
    "        exps = []\n",
    "        ids = []\n",
    "\n",
    "        for exp_id, start, end in exp_seps:\n",
    "            x_exp, y_exp = exp_data[:, start: end, :].copy()\n",
    "            exps.append({\"x\": x_exp, \"y\": y_exp})\n",
    "            ids.append(exp_id)\n",
    "\n",
    "        self.read_experiences(exps, ids)\n",
    "\n",
    "        for cluster_id, cluster in cluster_data.items():\n",
    "            updated_dict = {}\n",
    "            for key, value in cluster.items():\n",
    "                if key == \"members\":\n",
    "                    updated_dict[key] = set(value)\n",
    "                    for exp_id in value:\n",
    "                        self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "                elif type(value) == list:\n",
    "                    updated_dict[key] = np.array(value)\n",
    "            \n",
    "            cluster.update(updated_dict)\n",
    "            self.clusters[cluster_id] = cluster\n",
    "    \n",
    "\n",
    "    def build_adaptor(self):\n",
    "        total_size = 0\n",
    "        x_gps = []\n",
    "        y_gps = []\n",
    "        prior_weights = []\n",
    "        for cluster in self.clusters.values():\n",
    "            if cluster[\"data_size\"] >= 15:\n",
    "                total_size += cluster[\"size\"]\n",
    "\n",
    "        if not total_size:\n",
    "            raise Exception(\"Not enough data to build adaptor\")\n",
    "        \n",
    "        for cluster in self.clusters.values():\n",
    "            if cluster[\"data_size\"] < 15:\n",
    "                continue\n",
    "\n",
    "            prior_weights.append(cluster[\"size\"]/total_size)\n",
    "            x_gp = [copy.deepcopy(i) for i in self.x_priors[cluster[\"x_kernel_index\"]]]\n",
    "            y_gp = [copy.deepcopy(i) for i in self.y_priors[cluster[\"y_kernel_index\"]]]\n",
    "\n",
    "            if cluster[\"large\"]:\n",
    "                x_gp[1] = cluster[\"x_coef\"].copy()\n",
    "                y_gp[1] = cluster[\"y_coef\"].copy()\n",
    "\n",
    "            x_gps.append(x_gp)\n",
    "            y_gps.append(y_gp)\n",
    "        \n",
    "        adaptor = Adaptor_xy((x_gps, y_gps), np.array(prior_weights))\n",
    "        return adaptor\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_x = np.load(\"coor_x.npy\")\n",
    "coor_y = np.load(\"coor_y.npy\")\n",
    "baseline = np.load(\"filtered_baseline.npy\")\n",
    "\n",
    "# print(coor_x.shape)\n",
    "\n",
    "exps = []\n",
    "for file in os.listdir(r\"../exps/dmg1\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg1\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    x_targets = (data[:, 1] - baseline[indexes, 0]).reshape(-1, 1)\n",
    "    # print(baseline[indexes, 0])\n",
    "    x_coors = coor_x[indexes]\n",
    "    # print(x_targets.shape, x_coors.shape)\n",
    "    exp[\"x\"] = np.hstack((x_coors, x_targets))\n",
    "    y_targets = (data[:, 2] - baseline[indexes, 1]).reshape(-1, 1)\n",
    "    y_coors = coor_y[indexes]\n",
    "    exp[\"y\"] = np.hstack((y_coors, y_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg0\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg0\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    x_targets = (data[:, 1] - baseline[indexes, 0]).reshape(-1, 1)\n",
    "    x_coors = coor_x[indexes]\n",
    "    # print(x_targets.shape, x_coors.shape)\n",
    "    exp[\"x\"] = np.hstack((x_coors, x_targets))\n",
    "    y_targets = (data[:, 2] - baseline[indexes, 1]).reshape(-1, 1)\n",
    "    y_coors = coor_y[indexes]\n",
    "    exp[\"y\"] = np.hstack((y_coors, y_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg2\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg2\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    x_targets = (data[:, 1] - baseline[indexes, 0]).reshape(-1, 1)\n",
    "    x_coors = coor_x[indexes]\n",
    "    # print(x_targets.shape, x_coors.shape)\n",
    "    exp[\"x\"] = np.hstack((x_coors, x_targets))\n",
    "    y_targets = (data[:, 2] - baseline[indexes, 1]).reshape(-1, 1)\n",
    "    y_coors = coor_y[indexes]\n",
    "    exp[\"y\"] = np.hstack((y_coors, y_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg3\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg3\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    x_targets = (data[:, 1] - baseline[indexes, 0]).reshape(-1, 1)\n",
    "    x_coors = coor_x[indexes]\n",
    "    # print(x_targets.shape, x_coors.shape)\n",
    "    exp[\"x\"] = np.hstack((x_coors, x_targets))\n",
    "    y_targets = (data[:, 2] - baseline[indexes, 1]).reshape(-1, 1)\n",
    "    y_coors = coor_y[indexes]\n",
    "    exp[\"y\"] = np.hstack((y_coors, y_targets))\n",
    "\n",
    "    exps.append(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive = Archive_xy()\n",
    "\n",
    "prior_path = {\"x\": \"GP_x.npy\", \"y\": \"GP_y.npy\"}\n",
    "\n",
    "archive.load_priors(prior_path)\n",
    "\n",
    "archive.read_experiences(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(archive.next_exp_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.gibbs_sweep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(archive.clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    archive.gibbs_sweep()\n",
    "\n",
    "print(len(archive.clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 147\n",
      "10 131\n",
      "6 78\n",
      "4 49\n",
      "3 51\n",
      "10 126\n",
      "5 57\n",
      "2 20\n"
     ]
    }
   ],
   "source": [
    "for cluster in archive.clusters.values():\n",
    "    print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dmg(exp_id):\n",
    "    if exp_id < 11:\n",
    "        return 1\n",
    "    if exp_id < 31:\n",
    "        return 0\n",
    "    if exp_id < 36:\n",
    "        return 2\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.MAP(rounds=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "651.85724\n"
     ]
    }
   ],
   "source": [
    "print(archive.log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0 0.0    7\n",
      "1.0 0.0 0.0 0.0    11\n",
      "1.0 0.0 0.0 0.0    5\n",
      "0.0 0.0 0.0 1.0    11\n",
      "0.0 0.0 0.0 1.0    4\n",
      "1.0 0.0 0.0 0.0    2\n",
      "0.0 0.0 1.0 0.0    3\n",
      "0.0 1.0 0.0 0.0    3\n",
      "1.0 0.0 0.0 0.0    1\n",
      "1.0 0.0 0.0 0.0    1\n",
      "0.0 0.0 1.0 0.0    2\n",
      "(11, 4)\n"
     ]
    }
   ],
   "source": [
    "cluster_data = []\n",
    "for cluster in archive.clusters.values():\n",
    "    sub_data = np.zeros(4)\n",
    "    dmgs = [find_dmg(exp_id) for exp_id in cluster[\"members\"]]\n",
    "    print(dmgs.count(0) / len(dmgs), dmgs.count(1) / len(dmgs), dmgs.count(2) / len(dmgs), dmgs.count(3) / len(dmgs), \"  \", cluster[\"size\"])\n",
    "    sub_data[0] = dmgs.count(0)\n",
    "    sub_data[1] = dmgs.count(1)\n",
    "    sub_data[2] = dmgs.count(2)\n",
    "    sub_data[3] = dmgs.count(3)\n",
    "    cluster_data.append(sub_data.copy())\n",
    "\n",
    "    \n",
    "    if cluster[\"size\"] == 1:\n",
    "        for exp_id in cluster[\"members\"]:\n",
    "            if find_dmg(exp_id) == 2:\n",
    "                print(archive.exps[exp_id][\"data_size\"])\n",
    "\n",
    "cluster_data = np.vstack(cluster_data)\n",
    "print(cluster_data.shape)\n",
    "np.save(\"cluster_data_xy.npy\", cluster_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive.save()\n",
    "# del archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 96\n",
      "11 137\n",
      "5 72\n",
      "11 138\n",
      "4 45\n",
      "2 29\n",
      "3 51\n",
      "3 51\n",
      "1 10\n",
      "1 10\n",
      "2 20\n"
     ]
    }
   ],
   "source": [
    "for cluster in archive.clusters.values():\n",
    "    print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_archive = Archive_xy()\n",
    "# # new_archive.load_priors(prior_path)\n",
    "# new_archive.load(\"archive\")\n",
    "# for cluster in new_archive.clusters.values():\n",
    "#     print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
