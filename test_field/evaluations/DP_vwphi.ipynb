{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lg\n",
    "import matplotlib.pyplot as plt\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from Matern import Matern\n",
    "from Adaptor import Adaptor_arc\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archive(metaclass=ABCMeta):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load_priors(self, paths: dict):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_experiences(self, exps):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def gibbs_sweep(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def MAP(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, dir_path=None):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, dir_path):\n",
    "        pass\n",
    "\n",
    "    \n",
    "\n",
    "class Adaptor(metaclass=ABCMeta):\n",
    "\n",
    "    @abstractmethod\n",
    "    def read_data(self, data):\n",
    "        pass\n",
    "\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, coors):\n",
    "        pass\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Archive_arc(Archive):\n",
    "    def __init__(self, alpha=1.0, threshold=30):\n",
    "        self.alpha = alpha\n",
    "        self.kernel = Matern(v=1.5)\n",
    "        self.exps = {}\n",
    "        self.next_exp_id = 1\n",
    "        self.next_cluster_id = 1\n",
    "        self.empty_cluster_ids = []\n",
    "        self.empty_exp_ids = []\n",
    "        self.clusters = {}\n",
    "        self.threshold = threshold\n",
    "        self.backup_clusters = {}\n",
    "\n",
    "        self.v_priors = None\n",
    "        self.w_priors = None\n",
    "        self.phi_priors = None\n",
    "        self.log_prob = -np.inf\n",
    "\n",
    "    \n",
    "    def load_priors(self, paths):\n",
    "        v_priors = np.load(paths[\"v\"])\n",
    "        self.v_prior_data = v_priors.copy()\n",
    "        w_priors = np.load(paths[\"w\"])\n",
    "        self.w_prior_data = w_priors.copy()\n",
    "        phi_priors = np.load(paths[\"phi\"])\n",
    "        self.phi_prior_data = phi_priors.copy()\n",
    "\n",
    "\n",
    "        self.v_priors = []\n",
    "        for v_prior in v_priors:\n",
    "            coef = v_prior[:5].reshape(-1, 1)\n",
    "            length_scale = v_prior[5:10]\n",
    "            prior_var, noise_ratio = v_prior[10:12]\n",
    "            noisy = True if v_prior[-1] else False\n",
    "            self.v_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))\n",
    "        \n",
    "        self.w_priors = []\n",
    "        for w_prior in w_priors:\n",
    "            coef = w_prior[:5].reshape(-1, 1)\n",
    "            length_scale = w_prior[5:10]\n",
    "            prior_var, noise_ratio = w_prior[10:12]\n",
    "            noisy = True if w_prior[-1] else False\n",
    "            self.w_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))\n",
    "\n",
    "        self.phi_priors = []\n",
    "        for phi_prior in phi_priors:\n",
    "            coef = phi_prior[:5].reshape(-1, 1)\n",
    "            length_scale = phi_prior[5:10]\n",
    "            prior_var, noise_ratio = phi_prior[10:12]\n",
    "            noisy = True if phi_prior[-1] else False\n",
    "            self.phi_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))\n",
    "    \n",
    "\n",
    "\n",
    "    def read_experiences(self, exps, ids=None):\n",
    "        if ids is None and exps:\n",
    "            self.log_prob = -np.inf\n",
    "\n",
    "        for id_index, exp in enumerate(exps):\n",
    "            exp_dict = {}\n",
    "            v_data = exp[\"v\"]\n",
    "            w_data = exp[\"w\"]\n",
    "            phi_data = exp[\"phi\"]\n",
    "            exp_dict[\"data_size\"] = len(v_data)\n",
    "            exp_dict[\"v_data\"] = v_data.copy()\n",
    "            exp_dict[\"w_data\"] = w_data.copy()\n",
    "            exp_dict[\"phi_data\"] = phi_data.copy()\n",
    "\n",
    "            if not ids is None:\n",
    "                exp_id = ids[id_index]\n",
    "            elif self.empty_exp_ids:\n",
    "                exp_id = self.empty_exp_ids[0]\n",
    "                del self.empty_exp_ids[0]\n",
    "            else:\n",
    "                exp_id = self.next_exp_id\n",
    "                self.next_exp_id += 1\n",
    "\n",
    "            exp_dict[\"id\"] = exp_id\n",
    "            self.exps[exp_id] = exp_dict\n",
    "            v_loglikelihoods = np.zeros(len(self.v_priors), dtype=np.float32)\n",
    "            w_loglikelihoods = np.zeros(len(self.w_priors), dtype=np.float32)\n",
    "            phi_loglikelihoods = np.zeros(len(self.phi_priors), dtype=np.float32)\n",
    "\n",
    "            v_coors = v_data[:, :-2]\n",
    "            v_noise = v_data[:, -2]\n",
    "            v_target = v_data[:, -1].reshape(-1, 1)\n",
    "            w_coors = w_data[:, :-2]\n",
    "            w_noise = w_data[:, -2]\n",
    "            w_target = w_data[:, -1].reshape(-1, 1)\n",
    "            phi_coors = phi_data[:, :-2]\n",
    "            phi_noise = phi_data[:, -2]\n",
    "            phi_target = phi_data[:, -1].reshape(-1, 1)\n",
    "            \n",
    "            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.v_priors):\n",
    "                diff = v_target - v_coors @ coef\n",
    "                if noisy:\n",
    "                    v_loglikelihood = -0.5*(diff / (prior_var*v_noise.reshape(-1, 1)*noise_ratio + prior_var)).T @ diff - \\\n",
    "                        0.5*np.sum(np.log(prior_var*v_noise*noise_ratio + prior_var))\n",
    "                else:\n",
    "                    cov = self.kernel(v_coors, length_scales=length_scale) + np.diag(v_noise)*noise_ratio\n",
    "                    cov *= prior_var\n",
    "                    v_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)\n",
    "                    sign, log_det = lg.slogdet(cov)\n",
    "                    log_det = log_det if sign else -100\n",
    "                    v_loglikelihood -= 0.5*log_det\n",
    "                v_loglikelihoods[index] = float(v_loglikelihood)\n",
    "            \n",
    "\n",
    "            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.w_priors):\n",
    "                diff = w_target - w_coors @ coef\n",
    "                if noisy:\n",
    "                    w_loglikelihood = -0.5*(diff / (prior_var*w_noise.reshape(-1, 1)*noise_ratio + prior_var)).T @ diff - \\\n",
    "                        0.5*np.sum(np.log(prior_var*w_noise*noise_ratio + prior_var))\n",
    "                else:\n",
    "                    cov = self.kernel(w_coors, length_scales=length_scale) + np.diag(w_noise)*noise_ratio\n",
    "                    cov *= prior_var\n",
    "                    w_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)\n",
    "                    sign, log_det = lg.slogdet(cov)\n",
    "                    log_det = log_det if sign else -100\n",
    "                    w_loglikelihood -= 0.5*log_det\n",
    "                w_loglikelihoods[index] = float(w_loglikelihood)\n",
    "            \n",
    "\n",
    "            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.phi_priors):\n",
    "                diff = phi_target - phi_coors @ coef\n",
    "                if noisy:\n",
    "                    phi_loglikelihood = -0.5*(diff / (prior_var*phi_noise.reshape(-1, 1)*noise_ratio + prior_var)).T @ diff - \\\n",
    "                        0.5*np.sum(np.log(prior_var*phi_noise*noise_ratio + prior_var))\n",
    "                else:\n",
    "                    cov = self.kernel(phi_coors, length_scales=length_scale) + np.diag(phi_noise)*noise_ratio\n",
    "                    cov *= prior_var\n",
    "                    phi_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)\n",
    "                    sign, log_det = lg.slogdet(cov)\n",
    "                    log_det = log_det if sign else -100\n",
    "                    phi_loglikelihood -= 0.5*log_det\n",
    "                phi_loglikelihoods[index] = float(phi_loglikelihood)\n",
    "\n",
    "            exp_dict[\"v_loglikelihoods\"] = v_loglikelihoods\n",
    "            exp_dict[\"w_loglikelihoods\"] = w_loglikelihoods\n",
    "            exp_dict[\"phi_loglikelihoods\"] = phi_loglikelihoods\n",
    "            log_self_prob = np.log(np.mean(np.exp(v_loglikelihoods - np.max(v_loglikelihoods))))\n",
    "            log_self_prob += np.log(np.mean(np.exp(w_loglikelihoods - np.max(w_loglikelihoods))))\n",
    "            log_self_prob += np.log(np.mean(np.exp(phi_loglikelihoods - np.max(phi_loglikelihoods))))\n",
    "            log_self_prob += np.max(v_loglikelihoods) + np.max(w_loglikelihoods) + np.max(phi_loglikelihoods)\n",
    "            exp_dict[\"log_self_prob\"] = log_self_prob\n",
    "\n",
    "            if ids is None:\n",
    "                exp_dict[\"cluster\"] = self.alloc_cluster(exp_id)\n",
    "            else:\n",
    "                exp_dict[\"cluster\"] = 0\n",
    "        \n",
    "    \n",
    "\n",
    "    def drop_exp(self, exp_id):\n",
    "        exp = self.exps[exp_id]\n",
    "        self.remove_exp_from_cluster(exp_id, exp[\"cluster\"])\n",
    "        del self.exps[exp_id]\n",
    "        self.empty_exp_ids.append(exp_id)\n",
    "        self.log_prob = -np.inf\n",
    "\n",
    "\n",
    "\n",
    "    def alloc_cluster(self, exp_id):\n",
    "        if self.empty_cluster_ids:\n",
    "            cluster_id = self.empty_cluster_ids[0]\n",
    "            del self.empty_cluster_ids[0]\n",
    "        else:\n",
    "            cluster_id = self.next_cluster_id\n",
    "            self.next_cluster_id += 1\n",
    "        \n",
    "        cluster_dict = {\"id\": cluster_id}\n",
    "        cluster_dict[\"members\"] = {exp_id}\n",
    "        v_loglikelihoods = self.exps[exp_id][\"v_loglikelihoods\"].copy()\n",
    "        w_loglikelihoods = self.exps[exp_id][\"w_loglikelihoods\"].copy()\n",
    "        phi_loglikelihoods = self.exps[exp_id][\"phi_loglikelihoods\"].copy()\n",
    "        cluster_dict[\"v_loglikelihoods\"] = v_loglikelihoods\n",
    "        cluster_dict[\"w_loglikelihoods\"] = w_loglikelihoods\n",
    "        cluster_dict[\"phi_loglikelihoods\"] = phi_loglikelihoods\n",
    "        v_weights = np.exp(v_loglikelihoods - np.max(v_loglikelihoods))\n",
    "        w_weights = np.exp(w_loglikelihoods - np.max(w_loglikelihoods))\n",
    "        phi_weights = np.exp(phi_loglikelihoods - np.max(phi_loglikelihoods))\n",
    "        cluster_dict[\"v_weights\"] = v_weights / np.sum(v_weights)\n",
    "        cluster_dict[\"w_weights\"] = w_weights / np.sum(w_weights)\n",
    "        cluster_dict[\"phi_weights\"] = phi_weights / np.sum(phi_weights)\n",
    "        cluster_dict[\"size\"] = 1\n",
    "        cluster_dict[\"data_size\"] = self.exps[exp_id][\"data_size\"]\n",
    "        self.clusters[cluster_id] = cluster_dict\n",
    "        if cluster_dict[\"data_size\"] >= self.threshold:\n",
    "            cluster_dict[\"large\"] = True\n",
    "            cluster_dict[\"v_kernel_index\"] = int(np.argmax(v_weights))\n",
    "            cluster_dict[\"w_kernel_index\"] = int(np.argmax(w_weights))\n",
    "            cluster_dict[\"phi_kernel_index\"] = int(np.argmax(phi_weights))\n",
    "            v_coef, w_coef, phi_coef = self.fit_coef(cluster_id)\n",
    "            cluster_dict[\"v_coef\"] = v_coef\n",
    "            cluster_dict[\"w_coef\"] = w_coef\n",
    "            cluster_dict[\"phi_coef\"] = phi_coef\n",
    "        else:\n",
    "            cluster_dict[\"large\"] = False\n",
    "            cluster_dict[\"v_coef\"] = np.ones((5, 1), dtype=np.float32)\n",
    "            cluster_dict[\"w_coef\"] = np.ones((5, 1), dtype=np.float32)\n",
    "            cluster_dict[\"phi_coef\"] = np.ones((5, 1), dtype=np.float32)\n",
    "            cluster_dict[\"v_kernel_index\"] = 0\n",
    "            cluster_dict[\"w_kernel_index\"] = 0\n",
    "            cluster_dict[\"phi_kernel_index\"] = 0\n",
    "\n",
    "        return cluster_id\n",
    "\n",
    "    \n",
    "\n",
    "    def add_exp_to_cluster(self, exp_id, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        cluster[\"members\"].add(exp_id)\n",
    "        self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "        v_loglikelihoods = self.exps[exp_id][\"v_loglikelihoods\"]\n",
    "        w_loglikelihoods = self.exps[exp_id][\"w_loglikelihoods\"]\n",
    "        phi_loglikelihoods = self.exps[exp_id][\"phi_loglikelihoods\"]\n",
    "        cluster[\"v_loglikelihoods\"] += v_loglikelihoods\n",
    "        cluster[\"w_loglikelihoods\"] += w_loglikelihoods\n",
    "        cluster[\"phi_loglikelihoods\"] += phi_loglikelihoods\n",
    "        new_v_loglikelihoods = cluster[\"v_loglikelihoods\"]\n",
    "        new_w_loglikelihoods = cluster[\"w_loglikelihoods\"]\n",
    "        new_phi_loglikelihoods = cluster[\"phi_loglikelihoods\"]\n",
    "        v_weights = np.exp(new_v_loglikelihoods - np.max(new_v_loglikelihoods))\n",
    "        w_weights = np.exp(new_w_loglikelihoods - np.max(new_w_loglikelihoods))\n",
    "        phi_weights = np.exp(new_phi_loglikelihoods - np.max(new_phi_loglikelihoods))\n",
    "        cluster[\"v_weights\"] = v_weights / np.sum(v_weights)\n",
    "        cluster[\"w_weights\"] = w_weights / np.sum(w_weights)\n",
    "        cluster[\"phi_weights\"] = phi_weights / np.sum(phi_weights)\n",
    "        cluster[\"size\"] += 1\n",
    "        cluster[\"data_size\"] += self.exps[exp_id][\"data_size\"]\n",
    "\n",
    "        if cluster[\"data_size\"] >= self.threshold:\n",
    "            cluster[\"large\"] = True\n",
    "            cluster[\"v_kernel_index\"] = int(np.argmax(v_weights))\n",
    "            cluster[\"w_kernel_index\"] = int(np.argmax(w_weights))\n",
    "            cluster[\"phi_kernel_index\"] = int(np.argmax(phi_weights))\n",
    "            v_coef, w_coef, phi_coef = self.fit_coef(cluster_id)\n",
    "            cluster[\"v_coef\"] = v_coef\n",
    "            cluster[\"w_coef\"] = w_coef\n",
    "            cluster[\"phi_coef\"] = phi_coef\n",
    "\n",
    "\n",
    "\n",
    "    def remove_exp_from_cluster(self, exp_id, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        if cluster[\"size\"] == 1:\n",
    "            self.empty_cluster_ids.append(cluster_id)\n",
    "            del self.clusters[cluster_id]\n",
    "            return\n",
    "\n",
    "        cluster[\"members\"].discard(exp_id)\n",
    "        v_loglikelihoods = self.exps[exp_id][\"v_loglikelihoods\"]\n",
    "        w_loglikelihoods = self.exps[exp_id][\"w_loglikelihoods\"]\n",
    "        phi_loglikelihoods = self.exps[exp_id][\"phi_loglikelihoods\"]\n",
    "        cluster[\"v_loglikelihoods\"] -= v_loglikelihoods\n",
    "        cluster[\"w_loglikelihoods\"] -= w_loglikelihoods\n",
    "        cluster[\"phi_loglikelihoods\"] -= phi_loglikelihoods\n",
    "        new_v_loglikelihoods = cluster[\"v_loglikelihoods\"]\n",
    "        new_w_loglikelihoods = cluster[\"w_loglikelihoods\"]\n",
    "        new_phi_loglikelihoods = cluster[\"phi_loglikelihoods\"]\n",
    "        v_weights = np.exp(new_v_loglikelihoods - np.max(new_v_loglikelihoods))\n",
    "        w_weights = np.exp(new_w_loglikelihoods - np.max(new_w_loglikelihoods))\n",
    "        phi_weights = np.exp(new_phi_loglikelihoods - np.max(new_phi_loglikelihoods))\n",
    "        cluster[\"v_weights\"] = v_weights / np.sum(v_weights)\n",
    "        cluster[\"w_weights\"] = w_weights / np.sum(w_weights)\n",
    "        cluster[\"phi_weights\"] = w_weights / np.sum(phi_weights)\n",
    "        cluster[\"size\"] -= 1\n",
    "        cluster[\"data_size\"] -= self.exps[exp_id][\"data_size\"]\n",
    "\n",
    "        if cluster[\"data_size\"] >= self.threshold:\n",
    "            cluster[\"v_kernel_index\"] = int(np.argmax(v_weights))\n",
    "            cluster[\"w_kernel_index\"] = int(np.argmax(w_weights))\n",
    "            cluster[\"phi_kernel_index\"] = int(np.argmax(phi_weights))\n",
    "            v_coef, w_coef, phi_coef = self.fit_coef(cluster_id)\n",
    "            cluster[\"v_coef\"] = v_coef\n",
    "            cluster[\"w_coef\"] = w_coef\n",
    "            cluster[\"phi_coef\"] = phi_coef\n",
    "        else:\n",
    "            cluster[\"large\"] = False\n",
    "\n",
    "\n",
    "    \n",
    "    def fit_coef(self, cluster_id):\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        v_noisy, _, v_length_scale, __, v_noise_ratio = self.v_priors[cluster[\"v_kernel_index\"]]\n",
    "        w_noisy, _, w_length_scale, __, w_noise_ratio = self.w_priors[cluster[\"w_kernel_index\"]]\n",
    "        phi_noisy, _, phi_length_scale, __, phi_noise_ratio = self.phi_priors[cluster[\"phi_kernel_index\"]]\n",
    "\n",
    "        v_coors = [self.exps[exp_id][\"v_data\"][:, :-2] for exp_id in cluster[\"members\"]]\n",
    "        v_noises = [self.exps[exp_id][\"v_data\"][:, -2] for exp_id in cluster[\"members\"]]\n",
    "        v_targets = [self.exps[exp_id][\"v_data\"][:, -1].reshape(-1, 1) for exp_id in cluster[\"members\"]]\n",
    "\n",
    "        w_coors = [self.exps[exp_id][\"w_data\"][:, :-2] for exp_id in cluster[\"members\"]]\n",
    "        w_noises = [self.exps[exp_id][\"w_data\"][:, -2] for exp_id in cluster[\"members\"]]\n",
    "        w_targets = [self.exps[exp_id][\"w_data\"][:, -1].reshape(-1, 1) for exp_id in cluster[\"members\"]]\n",
    "\n",
    "        phi_coors = [self.exps[exp_id][\"phi_data\"][:, :-2] for exp_id in cluster[\"members\"]]\n",
    "        phi_noises = [self.exps[exp_id][\"phi_data\"][:, -2] for exp_id in cluster[\"members\"]]\n",
    "        phi_targets = [self.exps[exp_id][\"phi_data\"][:, -1].reshape(-1, 1) for exp_id in cluster[\"members\"]]\n",
    "\n",
    "        if v_noisy:\n",
    "            inverse_covs = [np.diag(1/(1 + v_noise*v_noise_ratio)) for v_noise in v_noises]\n",
    "        else:\n",
    "            inverse_covs = [lg.inv(\n",
    "                self.kernel(v_coor, length_scales=v_length_scale) + np.diag(v_noise)*v_noise_ratio\n",
    "                ) for v_coor, v_noise in zip(v_coors, v_noises)]\n",
    "            \n",
    "        A = sum([v_coor.T @ inverse_cov @ v_coor for v_coor, inverse_cov in zip(v_coors, inverse_covs)])\n",
    "        B = sum([v_coor.T @ inverse_cov @ v_target for v_coor, v_target, inverse_cov in zip(v_coors, v_targets, inverse_covs)])\n",
    "        v_coef = lg.inv(A) @ B\n",
    "\n",
    "        if w_noisy:\n",
    "            inverse_covs = [np.diag(1/(1 + w_noise*w_noise_ratio)) for w_noise in w_noises]\n",
    "        else:\n",
    "            inverse_covs = [lg.inv(\n",
    "                self.kernel(w_coor, length_scales=w_length_scale) + np.diag(w_noise)*w_noise_ratio\n",
    "                ) for w_coor, w_noise in zip(w_coors, w_noises)]\n",
    "            \n",
    "        A = sum([w_coor.T @ inverse_cov @ w_coor for w_coor, inverse_cov in zip(w_coors, inverse_covs)])\n",
    "        B = sum([w_coor.T @ inverse_cov @ w_target for w_coor, w_target, inverse_cov in zip(w_coors, w_targets, inverse_covs)])\n",
    "        w_coef = lg.inv(A) @ B\n",
    "\n",
    "        if phi_noisy:\n",
    "            inverse_covs = [np.diag(1/(1 + phi_noise*phi_noise_ratio)) for phi_noise in phi_noises]\n",
    "        else:\n",
    "            inverse_covs = [lg.inv(\n",
    "                self.kernel(phi_coor, length_scales=phi_length_scale) + np.diag(phi_noise)*phi_noise_ratio\n",
    "                ) for phi_coor, phi_noise in zip(phi_coors, phi_noises)]\n",
    "            \n",
    "        A = sum([phi_coor.T @ inverse_cov @ phi_coor for phi_coor, inverse_cov in zip(phi_coors, inverse_covs)])\n",
    "        B = sum([phi_coor.T @ inverse_cov @ phi_target for phi_coor, phi_target, inverse_cov in zip(phi_coors, phi_targets, inverse_covs)])\n",
    "        phi_coef = lg.inv(A) @ B\n",
    "\n",
    "        return v_coef, w_coef, phi_coef\n",
    "\n",
    "\n",
    "\n",
    "    def gibbs_sweep(self, return_log_prob=False):\n",
    "        for exp_id in self.exps:\n",
    "            exp = self.exps[exp_id]\n",
    "            log_probs = np.zeros(len(self.clusters)+1, dtype=np.float32)\n",
    "            clusters = np.zeros(len(self.clusters)+1, dtype=np.int32)\n",
    "            log_probs[0] = exp[\"log_self_prob\"] + math.log(self.alpha)\n",
    "            for index, cluster_id in enumerate(self.clusters, 1):\n",
    "                solo_cluster, log_prob = self.calculate_likelihood(exp_id, cluster_id)\n",
    "                if solo_cluster:\n",
    "                    log_probs[index] = log_probs[0]\n",
    "                    log_probs[0] = -np.inf\n",
    "                elif exp[\"cluster\"] == cluster_id:\n",
    "                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id][\"size\"] - 1)\n",
    "                else:\n",
    "                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id][\"size\"])\n",
    "\n",
    "                clusters[index] = cluster_id\n",
    "            \n",
    "            log_probs -= np.max(log_probs)\n",
    "            probs = np.exp(log_probs)\n",
    "            probs /= np.sum(probs)\n",
    "            selected_cluster = np.random.choice(clusters, p=probs)\n",
    "            \n",
    "            original_cluster_id = exp[\"cluster\"]\n",
    "            if selected_cluster != original_cluster_id:\n",
    "                self.remove_exp_from_cluster(exp_id, original_cluster_id)\n",
    "                if selected_cluster:\n",
    "                    self.add_exp_to_cluster(exp_id, int(selected_cluster))\n",
    "                else:\n",
    "                    exp[\"cluster\"] = self.alloc_cluster(exp_id)\n",
    "        \n",
    "        if not return_log_prob:\n",
    "            return \n",
    "        \n",
    "        log_prob = 0\n",
    "        for cluster in self.clusters.values():\n",
    "            log_prob += math.lgamma(cluster[\"size\"]) + math.log(self.alpha)\n",
    "            if cluster[\"large\"]:\n",
    "                v_noisy, _, v_length_scale, v_prior_var, v_noise_ratio = self.v_priors[cluster[\"v_kernel_index\"]]\n",
    "                w_noisy, _, w_length_scale, w_prior_var, w_noise_ratio = self.w_priors[cluster[\"w_kernel_index\"]]\n",
    "                phi_noisy, _, phi_length_scale, phi_prior_var, phi_noise_ratio = self.phi_priors[cluster[\"phi_kernel_index\"]]\n",
    "\n",
    "                v_coef = cluster[\"v_coef\"]\n",
    "                w_coef = cluster[\"w_coef\"]\n",
    "                phi_coef = cluster[\"phi_coef\"]\n",
    "\n",
    "                for exp_id in cluster[\"members\"]:\n",
    "                    v_coor = self.exps[exp_id][\"v_data\"][:, :-2]\n",
    "                    v_noise = self.exps[exp_id][\"v_data\"][:, -2]\n",
    "                    v_target = self.exps[exp_id][\"v_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "                    w_coor = self.exps[exp_id][\"w_data\"][:, :-2]\n",
    "                    w_noise = self.exps[exp_id][\"w_data\"][:, -2]\n",
    "                    w_target = self.exps[exp_id][\"w_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "                    phi_coor = self.exps[exp_id][\"phi_data\"][:, :-2]\n",
    "                    phi_noise = self.exps[exp_id][\"phi_data\"][:, -2]\n",
    "                    phi_target = self.exps[exp_id][\"phi_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "                    v_diff = v_target - v_coor @ v_coef\n",
    "                    w_diff = w_target - w_coor @ w_coef\n",
    "                    phi_diff = phi_target - phi_coor @ phi_coef\n",
    "\n",
    "                    if v_noisy:\n",
    "                        v_loglikelihood = -0.5*(v_diff / (v_prior_var*v_noise.reshape(-1, 1)*v_noise_ratio + v_prior_var)\n",
    "                                                ).T @ v_diff - 0.5*np.sum(np.log(v_prior_var*v_noise*v_noise_ratio + v_prior_var))\n",
    "                    else:\n",
    "                        cov = v_prior_var*self.kernel(v_coor, length_scales=v_length_scale) + \\\n",
    "                            np.diag(v_noise)*v_noise_ratio*v_prior_var\n",
    "                        v_loglikelihood = -0.5*(v_diff.T @ lg.inv(cov) @ v_diff)\n",
    "                        sign, log_det = lg.slogdet(cov)\n",
    "                        log_det = log_det if sign else -100\n",
    "                        v_loglikelihood -= 0.5*log_det\n",
    "\n",
    "                    if w_noisy:\n",
    "                        w_loglikelihood = -0.5*(w_diff / (w_prior_var*w_noise.reshape(-1, 1)*w_noise_ratio + w_prior_var)\n",
    "                                                ).T @ w_diff - 0.5*np.sum(np.log(w_prior_var*w_noise*w_noise_ratio + w_prior_var))\n",
    "                    else:\n",
    "                        cov = w_prior_var*self.kernel(w_coor, length_scales=w_length_scale) + \\\n",
    "                            np.diag(w_noise)*w_noise_ratio*w_prior_var\n",
    "                        w_loglikelihood = -0.5*(w_diff.T @ lg.inv(cov) @ w_diff)\n",
    "                        sign, log_det = lg.slogdet(cov)\n",
    "                        log_det = log_det if sign else -100\n",
    "                        w_loglikelihood -= 0.5*log_det\n",
    "                    \n",
    "                    if phi_noisy:\n",
    "                        phi_loglikelihood = -0.5*(phi_diff / (phi_prior_var*phi_noise.reshape(-1, 1)*phi_noise_ratio + phi_prior_var)\n",
    "                                                  ).T @ phi_diff - 0.5*np.sum(np.log(phi_prior_var*phi_noise*phi_noise_ratio + phi_prior_var))\n",
    "                    else:\n",
    "                        cov = phi_prior_var*self.kernel(phi_coor, length_scales=phi_length_scale) + \\\n",
    "                            np.diag(phi_noise)*phi_noise_ratio*phi_prior_var\n",
    "                        phi_loglikelihood = -0.5*(phi_diff.T @ lg.inv(cov) @ phi_diff)\n",
    "                        sign, log_det = lg.slogdet(cov)\n",
    "                        log_det = log_det if sign else -100\n",
    "                        phi_loglikelihood -= 0.5*log_det\n",
    "\n",
    "                    log_prob += float(v_loglikelihood + w_loglikelihood + phi_loglikelihood)\n",
    "            else:\n",
    "                v_log_likelihoods = cluster[\"v_loglikelihoods\"]\n",
    "                v_weights = cluster[\"v_weights\"]\n",
    "                log_prob += np.max(v_log_likelihoods)\n",
    "                log_prob += np.log(np.sum(v_weights * np.exp(v_log_likelihoods - np.max(v_log_likelihoods))))\n",
    "\n",
    "                w_log_likelihoods = cluster[\"w_loglikelihoods\"]\n",
    "                w_weights = cluster[\"w_weights\"]\n",
    "                log_prob += np.max(w_log_likelihoods)\n",
    "                log_prob += np.log(np.sum(w_weights * np.exp(w_log_likelihoods - np.max(w_log_likelihoods))))\n",
    "\n",
    "                phi_log_likelihoods = cluster[\"phi_loglikelihoods\"]\n",
    "                phi_weights = cluster[\"phi_weights\"]\n",
    "                log_prob += np.max(phi_log_likelihoods)\n",
    "                log_prob += np.log(np.sum(phi_weights * np.exp(phi_log_likelihoods - np.max(phi_log_likelihoods))))\n",
    "\n",
    "        return np.float32(log_prob)\n",
    "        \n",
    "\n",
    "\n",
    "    def calculate_likelihood(self, exp_id, cluster_id):\n",
    "        exp = self.exps[exp_id]\n",
    "        cluster = self.clusters[cluster_id]\n",
    "        solo_cluster = False\n",
    "\n",
    "        if exp[\"cluster\"] == cluster_id and cluster[\"size\"] == 1:\n",
    "            solo_cluster = True\n",
    "            log_prob = 0\n",
    "        elif exp[\"cluster\"] == cluster_id and (cluster[\"data_size\"] - exp[\"data_size\"]) < self.threshold:\n",
    "            v_loglikelihoods = cluster[\"v_loglikelihoods\"] - exp[\"v_loglikelihoods\"]\n",
    "            w_loglikelihoods = cluster[\"w_loglikelihoods\"] - exp[\"w_loglikelihoods\"]\n",
    "            phi_loglikelihoods = cluster[\"phi_loglikelihoods\"] - exp[\"phi_loglikelihoods\"]\n",
    "\n",
    "            v_weights = np.exp(v_loglikelihoods - np.max(v_loglikelihoods))\n",
    "            w_weights = np.exp(w_loglikelihoods - np.max(w_loglikelihoods))\n",
    "            phi_weights = np.exp(phi_loglikelihoods - np.max(phi_loglikelihoods))\n",
    "            v_weights /= np.sum(v_weights)\n",
    "            w_weights /= np.sum(w_weights)\n",
    "            phi_weights /= np.sum(phi_weights)\n",
    "\n",
    "            v_log_scale = np.max(exp[\"v_loglikelihoods\"])\n",
    "            w_log_scale = np.max(exp[\"w_loglikelihoods\"])\n",
    "            phi_log_scale = np.max(exp[\"phi_loglikelihoods\"])\n",
    "            v_prob = np.sum(v_weights * np.exp(exp[\"v_loglikelihoods\"] - v_log_scale))\n",
    "            w_prob = np.sum(w_weights * np.exp(exp[\"w_loglikelihoods\"] - w_log_scale))\n",
    "            phi_prob = np.sum(phi_weights * np.exp(exp[\"phi_loglikelihoods\"] - phi_log_scale))\n",
    "\n",
    "            prob = (math.log(v_prob) if v_prob else -40) + (math.log(w_prob) if w_prob else -40) + (math.log(phi_prob) if phi_prob else -40) \n",
    "            log_prob = v_log_scale + w_log_scale + phi_log_scale + prob\n",
    "        elif not cluster[\"large\"]:\n",
    "            v_log_scale = np.max(exp[\"v_loglikelihoods\"])\n",
    "            w_log_scale = np.max(exp[\"w_loglikelihoods\"])\n",
    "            phi_log_scale = np.max(exp[\"phi_loglikelihoods\"])\n",
    "            v_prob = np.sum(cluster[\"v_weights\"] * np.exp(exp[\"v_loglikelihoods\"] - v_log_scale))\n",
    "            w_prob = np.sum(cluster[\"w_weights\"] * np.exp(exp[\"w_loglikelihoods\"] - w_log_scale))\n",
    "            phi_prob = np.sum(cluster[\"phi_weights\"] * np.exp(exp[\"phi_loglikelihoods\"] - phi_log_scale))\n",
    "            prob = (math.log(v_prob) if v_prob else -40) + (math.log(w_prob) if w_prob else -40) + (math.log(phi_prob) if phi_prob else -40) \n",
    "            log_prob = v_log_scale + w_log_scale + phi_log_scale + prob\n",
    "        else:\n",
    "            v_noisy, _, v_length_scale, v_prior_var, v_noise_ratio = self.v_priors[cluster[\"v_kernel_index\"]]\n",
    "            w_noisy, _, w_length_scale, w_prior_var, w_noise_ratio = self.w_priors[cluster[\"w_kernel_index\"]]\n",
    "            phi_noisy, _, phi_length_scale, phi_prior_var, phi_noise_ratio = self.phi_priors[cluster[\"phi_kernel_index\"]]\n",
    "\n",
    "            v_coor = self.exps[exp_id][\"v_data\"][:, :-2]\n",
    "            v_noise = self.exps[exp_id][\"v_data\"][:, -2]\n",
    "            v_target = self.exps[exp_id][\"v_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "            w_coor = self.exps[exp_id][\"w_data\"][:, :-2]\n",
    "            w_noise = self.exps[exp_id][\"w_data\"][:, -2]\n",
    "            w_target = self.exps[exp_id][\"w_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "            phi_coor = self.exps[exp_id][\"phi_data\"][:, :-2]\n",
    "            phi_noise = self.exps[exp_id][\"phi_data\"][:, -2]\n",
    "            phi_target = self.exps[exp_id][\"phi_data\"][:, -1].reshape(-1, 1)\n",
    "\n",
    "            v_diff = v_target - v_coor @ cluster[\"v_coef\"]\n",
    "            w_diff = w_target - w_coor @ cluster[\"w_coef\"]\n",
    "            phi_diff = phi_target - phi_coor @ cluster[\"phi_coef\"]\n",
    "\n",
    "            if v_noisy:\n",
    "                v_loglikelihood = -0.5*(v_diff / (v_prior_var*v_noise.reshape(-1, 1)*v_noise_ratio + v_prior_var)).T @ v_diff - \\\n",
    "                    0.5*np.sum(np.log(v_prior_var*v_noise*v_noise_ratio + v_prior_var))\n",
    "            else:\n",
    "                cov = v_prior_var*self.kernel(v_coor, length_scales=v_length_scale) + np.diag(v_noise)*v_noise_ratio*v_prior_var\n",
    "                v_loglikelihood = -0.5*(v_diff.T @ lg.inv(cov) @ v_diff)\n",
    "                sign, log_det = lg.slogdet(cov)\n",
    "                log_det = log_det if sign else -100\n",
    "                v_loglikelihood -= 0.5*log_det\n",
    "\n",
    "            if w_noisy:\n",
    "                w_loglikelihood = -0.5*(w_diff / (w_prior_var*w_noise.reshape(-1, 1)*w_noise_ratio + w_prior_var)).T @ w_diff - \\\n",
    "                    0.5*np.sum(np.log(w_prior_var*w_noise*w_noise_ratio + w_prior_var))\n",
    "            else:\n",
    "                cov = w_prior_var*self.kernel(w_coor, length_scales=w_length_scale) + np.diag(w_noise)*w_noise_ratio*w_prior_var\n",
    "                w_loglikelihood = -0.5*(w_diff.T @ lg.inv(cov) @ w_diff)\n",
    "                sign, log_det = lg.slogdet(cov)\n",
    "                log_det = log_det if sign else -100\n",
    "                w_loglikelihood -= 0.5*log_det\n",
    "            \n",
    "            if phi_noisy:\n",
    "                phi_loglikelihood = -0.5*(phi_diff / (phi_prior_var*phi_noise.reshape(-1, 1)*phi_noise_ratio + phi_prior_var)).T @ phi_diff - \\\n",
    "                    0.5*np.sum(np.log(phi_prior_var*phi_noise*phi_noise_ratio + phi_prior_var))\n",
    "            else:\n",
    "                cov = phi_prior_var*self.kernel(phi_coor, length_scales=phi_length_scale) + np.diag(phi_noise)*phi_noise_ratio*phi_prior_var\n",
    "                phi_loglikelihood = -0.5*(phi_diff.T @ lg.inv(cov) @ phi_diff)\n",
    "                sign, log_det = lg.slogdet(cov)\n",
    "                log_det = log_det if sign else -100\n",
    "                phi_loglikelihood -= 0.5*log_det\n",
    "\n",
    "            log_prob = v_loglikelihood + w_loglikelihood + phi_loglikelihood\n",
    "\n",
    "        return solo_cluster, log_prob\n",
    "\n",
    "\n",
    "    def freeze(self):\n",
    "        self.backup_clusters = {int(key): {i: copy.deepcopy(j) for i, j in value.items()} for key, value in self.clusters.items()}\n",
    "        self.backup_next_cluster_id = np.int32(self.next_cluster_id)\n",
    "        self.backup_empty_cluster_ids = self.empty_cluster_ids.copy()\n",
    "    \n",
    "\n",
    "\n",
    "    def MAP(self, rounds=20):\n",
    "        for r in range(rounds):\n",
    "            new_log_prob = self.gibbs_sweep(return_log_prob=True)\n",
    "            if new_log_prob > self.log_prob:\n",
    "                self.freeze()\n",
    "                self.log_prob = new_log_prob\n",
    "        \n",
    "        self.clusters = {int(key): {i: copy.deepcopy(j) for i, j in value.items()} for key, value in self.backup_clusters.items()}\n",
    "        self.next_cluster_id = int(self.backup_next_cluster_id)\n",
    "        self.empty_cluster_ids = [i for i in self.backup_empty_cluster_ids]\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            for exp_id in cluster[\"members\"]:\n",
    "                self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "\n",
    "\n",
    "\n",
    "    def save(self, dir_path=None):\n",
    "        dir_path = \"archive_arc\" if dir_path is None else dir_path\n",
    "        base_configs = {\"alpha\": self.alpha, \n",
    "                        \"next_exp_id\": self.next_exp_id, \n",
    "                        \"next_cluster_id\": self.next_cluster_id, \n",
    "                        \"empty_cluster_ids\": self.empty_cluster_ids.copy(), \n",
    "                        \"empty_exp_ids\": self.empty_exp_ids.copy(), \n",
    "                        \"threshold\": self.threshold} # <--\n",
    "        \n",
    "        exp_seps = [] \n",
    "        exp_length = 0\n",
    "        exps_v_data = []\n",
    "        exps_w_data = []\n",
    "        exps_phi_data = []\n",
    "\n",
    "        for exp_id, exp in self.exps.items():\n",
    "            exp_seps.append((exp_id, exp_length, exp_length + exp[\"data_size\"]))\n",
    "            exp_length += exp[\"data_size\"]\n",
    "            exps_v_data.append(exp[\"v_data\"])\n",
    "            exps_w_data.append(exp[\"w_data\"])\n",
    "            exps_phi_data.append(exp[\"phi_data\"])\n",
    "        base_configs[\"exp_seps\"] = exp_seps\n",
    "        \n",
    "        exps_v_data = np.concatenate(exps_v_data, axis=0)[None, :, :]\n",
    "        exps_w_data = np.concatenate(exps_w_data, axis=0)[None, :, :]\n",
    "        exps_phi_data = np.concatenate(exps_phi_data, axis=0)[None, :, :]\n",
    "        exp_data = np.concatenate((exps_v_data, exps_w_data, exps_phi_data), axis=0) # <--\n",
    "\n",
    "        clusters_data = {} # <--\n",
    "        for cluster_id, cluster in self.clusters.items():\n",
    "            cluster_info = {}\n",
    "            for key, value in cluster.items():\n",
    "                if type(value) == np.ndarray:\n",
    "                    cluster_info[key] = value.tolist()\n",
    "                elif type(value) == set:\n",
    "                    cluster_info[key] = list(value)\n",
    "                else:\n",
    "                    cluster_info[key] = value\n",
    "            \n",
    "            clusters_data[cluster_id] = cluster_info\n",
    "        \n",
    "        if not os.path.exists(dir_path):\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "        np.save(os.path.join(dir_path, \"exp_data.npy\"), exp_data)\n",
    "        np.save(os.path.join(dir_path, \"GP_v.npy\"), self.v_prior_data)\n",
    "        np.save(os.path.join(dir_path, \"GP_w.npy\"), self.w_prior_data)\n",
    "        np.save(os.path.join(dir_path, \"GP_phi.npy\"), self.phi_prior_data)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"base_configs.json\"), \"w\") as file:\n",
    "            json.dump(base_configs, file)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"cluster_data.json\"), \"w\") as file:\n",
    "            json.dump(clusters_data, file)\n",
    "        \n",
    "\n",
    "\n",
    "    def load(self, dir_path):\n",
    "        prior_path = {\"v\": os.path.join(dir_path, \"GP_v.npy\"), \n",
    "                      \"w\": os.path.join(dir_path, \"GP_w.npy\"),\n",
    "                      \"phi\": os.path.join(dir_path, \"GP_phi.npy\")}\n",
    "        self.load_priors(prior_path)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"base_configs.json\"), \"r\") as file:\n",
    "            base_configs = json.load(file)\n",
    "\n",
    "        with open(os.path.join(dir_path, \"cluster_data.json\"), \"r\") as file:\n",
    "            cluster_data = json.load(file)\n",
    "        \n",
    "        exp_data = np.load(os.path.join(dir_path, \"exp_data.npy\"))\n",
    "\n",
    "        self.alpha = base_configs[\"alpha\"]\n",
    "        self.exps = {}\n",
    "        self.clusters = {}\n",
    "        self.next_exp_id = base_configs[\"next_exp_id\"]\n",
    "        self.next_cluster_id = base_configs[\"next_cluster_id\"]\n",
    "        self.empty_cluster_ids = base_configs[\"empty_cluster_ids\"]\n",
    "        self.empty_exp_ids = base_configs[\"empty_exp_ids\"]\n",
    "        self.threshold = base_configs[\"threshold\"]\n",
    "\n",
    "        exp_seps = base_configs[\"exp_seps\"]\n",
    "        exps = []\n",
    "        ids = []\n",
    "\n",
    "        for exp_id, start, end in exp_seps:\n",
    "            v_exp, w_exp, phi_exp = exp_data[:, start: end, :].copy()\n",
    "            exps.append({\"v\": v_exp, \"w\": w_exp, \"phi\": phi_exp})\n",
    "            ids.append(exp_id)\n",
    "\n",
    "        self.read_experiences(exps, ids)\n",
    "\n",
    "        for cluster_id, cluster in cluster_data.items():\n",
    "            updated_dict = {}\n",
    "            for key, value in cluster.items():\n",
    "                if key == \"members\":\n",
    "                    updated_dict[key] = set(value)\n",
    "                    for exp_id in value:\n",
    "                        self.exps[exp_id][\"cluster\"] = cluster_id\n",
    "                elif type(value) == list:\n",
    "                    updated_dict[key] = np.array(value)\n",
    "            \n",
    "            cluster.update(updated_dict)\n",
    "            self.clusters[cluster_id] = cluster\n",
    "    \n",
    "\n",
    "    def build_adaptor(self):\n",
    "        total_size = 0\n",
    "        v_gps = []\n",
    "        w_gps = []\n",
    "        phi_gps = []\n",
    "        prior_weights = []\n",
    "        for cluster in self.clusters.values():\n",
    "            if cluster[\"data_size\"] >= 15:\n",
    "                total_size += cluster[\"size\"]\n",
    "\n",
    "        if not total_size:\n",
    "            raise Exception(\"Not enough data to build adaptor\")\n",
    "        \n",
    "        for cluster in self.clusters.values():\n",
    "            if cluster[\"data_size\"] < 15:\n",
    "                continue\n",
    "\n",
    "            prior_weights.append(cluster[\"size\"]/total_size)\n",
    "            v_gp = [copy.deepcopy(i) for i in self.v_priors[cluster[\"v_kernel_index\"]]]\n",
    "            w_gp = [copy.deepcopy(i) for i in self.w_priors[cluster[\"w_kernel_index\"]]]\n",
    "            phi_gp = [copy.deepcopy(i) for i in self.phi_priors[cluster[\"phi_kernel_index\"]]]\n",
    "\n",
    "            if cluster[\"large\"]:\n",
    "                v_gp[1] = cluster[\"v_coef\"].copy()\n",
    "                w_gp[1] = cluster[\"w_coef\"].copy()\n",
    "                phi_gp[1] = cluster[\"phi_coef\"].copy()\n",
    "\n",
    "            v_gps.append(v_gp)\n",
    "            w_gps.append(w_gp)\n",
    "            phi_gps.append(phi_gp)\n",
    "        \n",
    "        adaptor = Adaptor_arc((v_gps, w_gps, phi_gps), np.array(prior_weights))\n",
    "        return adaptor\n",
    "                    \n",
    "\n",
    "    \n",
    "# a = np.zeros((2, 4))\n",
    "# print(type(a))\n",
    "\n",
    "# print(os.path.exists(\"coor_x.npy\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "coor_v = np.load(\"coor_v.npy\")\n",
    "coor_w = np.load(\"coor_w.npy\")\n",
    "coor_phi = np.load(\"coor_phi.npy\")\n",
    "baseline = np.load(\"filtered_baseline.npy\")\n",
    "\n",
    "# print(coor_x.shape)\n",
    "\n",
    "exps = []\n",
    "for file in os.listdir(r\"../exps/dmg1\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg1\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "\n",
    "    v_targets = (data[:, 3] - baseline[indexes, 2]).reshape(-1, 1)\n",
    "    v_coors = coor_v[indexes]\n",
    "    w_targets = (data[:, 4] - baseline[indexes, 3]).reshape(-1, 1)\n",
    "    w_coors = coor_w[indexes]\n",
    "    phi_targets = np.mod((data[:, 5] - baseline[indexes, 4] + 3*np.pi), 2*np.pi).reshape(-1, 1) - np.pi\n",
    "    phi_coors = coor_phi[indexes]\n",
    "\n",
    "    v = data[:, 3]\n",
    "    w = data[:, 4]\n",
    "    phi = data[:, 5]\n",
    "    sin_theta = np.sin(w*4 + phi)\n",
    "    cos_theta = np.cos(w*4 + phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "    cos_phi = np.cos(phi)\n",
    "    x_ = v/w*(sin_theta - sin_phi)\n",
    "    y_ = v/w*(cos_phi - cos_theta)\n",
    "\n",
    "    noise_v = v**2 / (x_**2 + y_**2 + 1e-6) + 1e-6\n",
    "    noise_w = (w**2 + 0.01) / (np.square(4*v*cos_theta - x_) + np.square(4*v*sin_theta - y_) + 1e-8)\n",
    "    noise_phi = 1 / (x_**2 + y_**2 + 16e-6*v**2)\n",
    "\n",
    "    exp[\"v\"] = np.hstack((v_coors, noise_v.reshape(-1, 1), v_targets))\n",
    "    exp[\"w\"] = np.hstack((w_coors, noise_w.reshape(-1, 1), w_targets))\n",
    "    exp[\"phi\"] = np.hstack((phi_coors, noise_phi.reshape(-1, 1), phi_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg0\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg0\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    v_targets = (data[:, 3] - baseline[indexes, 2]).reshape(-1, 1)\n",
    "    v_coors = coor_v[indexes]\n",
    "    w_targets = (data[:, 4] - baseline[indexes, 3]).reshape(-1, 1)\n",
    "    w_coors = coor_w[indexes]\n",
    "    phi_targets = np.mod((data[:, 5] - baseline[indexes, 4] + 3*np.pi), 2*np.pi).reshape(-1, 1) - np.pi\n",
    "    phi_coors = coor_phi[indexes]\n",
    "\n",
    "    v = data[:, 3]\n",
    "    w = data[:, 4]\n",
    "    phi = data[:, 5]\n",
    "    sin_theta = np.sin(w*4 + phi)\n",
    "    cos_theta = np.cos(w*4 + phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "    cos_phi = np.cos(phi)\n",
    "    x_ = v/w*(sin_theta - sin_phi)\n",
    "    y_ = v/w*(cos_phi - cos_theta)\n",
    "\n",
    "    noise_v = v**2 / (x_**2 + y_**2 + 1e-6) + 1e-6\n",
    "    noise_w = (w**2 + 0.01) / (np.square(4*v*cos_theta - x_) + np.square(4*v*sin_theta - y_) + 1e-8)\n",
    "    noise_phi = 1 / (x_**2 + y_**2 + 16e-6*v**2)\n",
    "\n",
    "    exp[\"v\"] = np.hstack((v_coors, noise_v.reshape(-1, 1), v_targets))\n",
    "    exp[\"w\"] = np.hstack((w_coors, noise_w.reshape(-1, 1), w_targets))\n",
    "    exp[\"phi\"] = np.hstack((phi_coors, noise_phi.reshape(-1, 1), phi_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg2\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg2\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "\n",
    "    v_targets = (data[:, 3] - baseline[indexes, 2]).reshape(-1, 1)\n",
    "    v_coors = coor_v[indexes]\n",
    "    w_targets = (data[:, 4] - baseline[indexes, 3]).reshape(-1, 1)\n",
    "    w_coors = coor_w[indexes]\n",
    "    phi_targets = np.mod((data[:, 5] - baseline[indexes, 4] + 3*np.pi), 2*np.pi).reshape(-1, 1) - np.pi\n",
    "    phi_coors = coor_phi[indexes]\n",
    "\n",
    "    v = data[:, 3]\n",
    "    w = data[:, 4]\n",
    "    phi = data[:, 5]\n",
    "    sin_theta = np.sin(w*4 + phi)\n",
    "    cos_theta = np.cos(w*4 + phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "    cos_phi = np.cos(phi)\n",
    "    x_ = v/w*(sin_theta - sin_phi)\n",
    "    y_ = v/w*(cos_phi - cos_theta)\n",
    "\n",
    "    noise_v = v**2 / (x_**2 + y_**2 + 1e-6) + 1e-6\n",
    "    noise_w = (w**2 + 0.01) / (np.square(4*v*cos_theta - x_) + np.square(4*v*sin_theta - y_) + 1e-8)\n",
    "    noise_phi = 1 / (x_**2 + y_**2 + 16e-6*v**2)\n",
    "\n",
    "    exp[\"v\"] = np.hstack((v_coors, noise_v.reshape(-1, 1), v_targets))\n",
    "    exp[\"w\"] = np.hstack((w_coors, noise_w.reshape(-1, 1), w_targets))\n",
    "    exp[\"phi\"] = np.hstack((phi_coors, noise_phi.reshape(-1, 1), phi_targets))\n",
    "\n",
    "    exps.append(exp)\n",
    "\n",
    "\n",
    "for file in os.listdir(r\"../exps/dmg3\"):\n",
    "    data = np.load(os.path.join(r\"../exps/dmg3\", file))\n",
    "    exp = {}\n",
    "    indexes = np.int32(data[:, 0])\n",
    "    v_targets = (data[:, 3] - baseline[indexes, 2]).reshape(-1, 1)\n",
    "    v_coors = coor_v[indexes]\n",
    "    w_targets = (data[:, 4] - baseline[indexes, 3]).reshape(-1, 1)\n",
    "    w_coors = coor_w[indexes]\n",
    "    phi_targets = np.mod((data[:, 5] - baseline[indexes, 4] + 3*np.pi), 2*np.pi).reshape(-1, 1) - np.pi\n",
    "    phi_coors = coor_phi[indexes]\n",
    "\n",
    "    v = data[:, 3]\n",
    "    w = data[:, 4]\n",
    "    phi = data[:, 5]\n",
    "    sin_theta = np.sin(w*4 + phi)\n",
    "    cos_theta = np.cos(w*4 + phi)\n",
    "    sin_phi = np.sin(phi)\n",
    "    cos_phi = np.cos(phi)\n",
    "    x_ = v/w*(sin_theta - sin_phi)\n",
    "    y_ = v/w*(cos_phi - cos_theta)\n",
    "\n",
    "    noise_v = v**2 / (x_**2 + y_**2 + 1e-6) + 1e-6\n",
    "    noise_w = (w**2 + 0.01) / (np.square(4*v*cos_theta - x_) + np.square(4*v*sin_theta - y_) + 1e-8)\n",
    "    noise_phi = 1 / (x_**2 + y_**2 + 16e-6*v**2)\n",
    "\n",
    "    exp[\"v\"] = np.hstack((v_coors, noise_v.reshape(-1, 1), v_targets))\n",
    "    exp[\"w\"] = np.hstack((w_coors, noise_w.reshape(-1, 1), w_targets))\n",
    "    exp[\"phi\"] = np.hstack((phi_coors, noise_phi.reshape(-1, 1), phi_targets))\n",
    "\n",
    "    exps.append(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# archive = Archive_xy()\n",
    "archive = Archive_arc()\n",
    "\n",
    "prior_path = {\"v\": \"GP_v.npy\", \"w\": \"GP_w.npy\", \"phi\": \"GP_phi.npy\"}\n",
    "\n",
    "archive.load_priors(prior_path)\n",
    "\n",
    "archive.read_experiences(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(archive.next_exp_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.gibbs_sweep()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "print(len(archive.clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    archive.gibbs_sweep()\n",
    "\n",
    "print(len(archive.clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 80\n",
      "4 43\n",
      "2 33\n",
      "9 95\n",
      "1 16\n",
      "1 18\n",
      "5 72\n",
      "4 57\n",
      "4 62\n",
      "9 120\n",
      "1 9\n",
      "1 10\n",
      "4 44\n"
     ]
    }
   ],
   "source": [
    "for cluster in archive.clusters.values():\n",
    "    print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dmg(exp_id):\n",
    "    if exp_id < 11:\n",
    "        return 1\n",
    "    if exp_id < 31:\n",
    "        return 0\n",
    "    if exp_id < 36:\n",
    "        return 2\n",
    "    return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.MAP(rounds=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1934.0901\n"
     ]
    }
   ],
   "source": [
    "print(archive.log_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1.0 0.0 0.0\n",
      "0.0 0.75 0.0 0.25\n",
      "0.0 1.0 0.0 0.0\n",
      "1.0 0.0 0.0 0.0\n",
      "1.0 0.0 0.0 0.0\n",
      "1.0 0.0 0.0 0.0\n",
      "1.0 0.0 0.0 0.0\n",
      "1.0 0.0 0.0 0.0\n",
      "0.0 0.0 1.0 0.0\n",
      "0.0 0.0 0.0 1.0\n",
      "0.0 0.0 1.0 0.0\n",
      "0.0 0.0 0.0 1.0\n",
      "0.0 0.0 0.0 1.0\n",
      "0.0 0.0 0.0 1.0\n"
     ]
    }
   ],
   "source": [
    "for cluster in archive.clusters.values():\n",
    "    dmgs = [find_dmg(exp_id) for exp_id in cluster[\"members\"]]\n",
    "    print(dmgs.count(0) / len(dmgs), dmgs.count(1) / len(dmgs), dmgs.count(2) / len(dmgs), dmgs.count(3) / len(dmgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive.save()\n",
    "# del archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 80\n",
      "4 43\n",
      "2 33\n",
      "9 95\n",
      "1 16\n",
      "1 18\n",
      "5 72\n",
      "4 57\n",
      "3 51\n",
      "5 73\n",
      "2 20\n",
      "2 18\n",
      "5 51\n",
      "2 32\n"
     ]
    }
   ],
   "source": [
    "for cluster in archive.clusters.values():\n",
    "    print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for cluster in archive.clusters.values():\n",
    "#     print(cluster[\"size\"], cluster[\"data_size\"], np.max(cluster[\"phi_loglikelihoods\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_archive = Archive_arc()\n",
    "# # # new_archive.load_priors(prior_path)\n",
    "# new_archive.load(\"archive_arc\")\n",
    "# for cluster in new_archive.clusters.values():\n",
    "#     print(cluster[\"size\"], cluster[\"data_size\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(new_archive.log_prob)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
