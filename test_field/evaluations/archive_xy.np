import numpy as np
import numpy.linalg as lg
import matplotlib.pyplot as plt
from abc import ABCMeta, abstractmethod
from Matern import Matern
import math
import os
import copy
import json


class Archive_xy:
    def __init__(self, alpha=1.0, threshold=40):
        self.alpha = alpha
        self.kernel = Matern(v=1.5)
        self.exps = {}
        self.next_exp_id = 1
        self.next_cluster_id = 1
        self.empty_cluster_ids = []
        self.empty_exp_ids = []
        self.clusters = {}
        self.threshold = threshold
        self.backup_clusters = {}
        self.x_priors = None
        self.y_priors = None

    
    def load_priors(self, paths):
        x_priors = np.load(paths["x"])
        self.x_prior_data = x_priors.copy()
        y_priors = np.load(paths["y"])
        self.y_prior_data = y_priors.copy()

        self.x_priors = []
        for x_prior in x_priors:
            coef = x_prior[:5].reshape(-1, 1)
            length_scale = x_prior[5:10]
            prior_var, noise_ratio = x_prior[10:12]
            noisy = True if x_prior[-1] else False
            self.x_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))
        
        self.y_priors = []
        for y_prior in y_priors:
            coef = y_prior[:5].reshape(-1, 1)
            length_scale = y_prior[5:10]
            prior_var, noise_ratio = y_prior[10:12]
            noisy = True if y_prior[-1] else False
            self.y_priors.append((noisy, coef, length_scale, prior_var, noise_ratio))
    

    def read_experiences(self, exps, ids=None):
        for id_index, exp in enumerate(exps):
            exp_dict = {}
            x_data = exp["x"]
            y_data = exp["y"]
            exp_dict["data_size"] = len(x_data)
            exp_dict["x_data"] = x_data.copy()
            exp_dict["y_data"] = y_data.copy()

            if not ids is None:
                exp_id = ids[id_index]
            elif self.empty_exp_ids:
                exp_id = self.empty_exp_ids[0]
                del self.empty_exp_ids[0]
            else:
                exp_id = self.next_exp_id
                self.next_exp_id += 1

            exp_dict["id"] = exp_id
            self.exps[exp_id] = exp_dict
            x_loglikelihoods = np.zeros(len(self.x_priors), dtype=np.float32)
            y_loglikelihoods = np.zeros(len(self.y_priors), dtype=np.float32)

            x_coors = x_data[:, :-1]
            x_target = x_data[:, -1].reshape(-1, 1)
            y_coors = y_data[:, :-1]
            y_target = y_data[:, -1].reshape(-1, 1)
            
            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.x_priors):
                diff = x_target - x_coors @ coef
                data_size = len(x_data)
                if noisy:
                    x_loglikelihood = -0.5*(diff / (prior_var*noise_ratio + prior_var)).T @ diff - \
                        0.5*data_size*math.log(prior_var*noise_ratio + prior_var)
                else:
                    cov = self.kernel(x_coors, length_scales=length_scale) + np.eye(data_size)*noise_ratio
                    cov *= prior_var
                    x_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)
                    sign, log_det = lg.slogdet(cov)
                    log_det = log_det if sign else -100
                    x_loglikelihood -= 0.5*log_det
                x_loglikelihoods[index] = float(x_loglikelihood)

            for index, (noisy, coef, length_scale, prior_var, noise_ratio) in enumerate(self.y_priors):
                diff = y_target - y_coors @ coef
                data_size = len(y_data)
                if noisy:
                    y_loglikelihood = -0.5*(diff / (prior_var*noise_ratio + prior_var)).T @ diff - \
                        0.5*data_size*math.log(prior_var*noise_ratio + prior_var)
                else:
                    cov = self.kernel(y_coors, length_scales=length_scale) + np.eye(data_size)*noise_ratio
                    cov *= prior_var
                    y_loglikelihood = -0.5*(diff.T @ lg.inv(cov) @ diff)
                    sign, log_det = lg.slogdet(cov)
                    log_det = log_det if sign else -100
                    y_loglikelihood -= 0.5*log_det
                y_loglikelihoods[index] = float(y_loglikelihood)
            exp_dict["x_loglikelihoods"] = x_loglikelihoods
            exp_dict["y_loglikelihoods"] = y_loglikelihoods
            log_self_prob = np.log(np.mean(
                np.exp(x_loglikelihoods - np.max(x_loglikelihoods))
                )*np.mean(np.exp(y_loglikelihoods - np.max(y_loglikelihoods))))
            log_self_prob += np.max(x_loglikelihoods) + np.max(y_loglikelihoods)
            exp_dict["log_self_prob"] = log_self_prob

            if ids is None:
                exp_dict["cluster"] = self.alloc_cluster(exp_id)
            else:
                exp_dict["cluster"] = 0
    


    def drop_exp(self, exp_id):
        exp = self.exps[exp_id]
        self.remove_exp_from_cluster(exp_id, exp["cluster"])
        del self.exps[exp_id]
        self.empty_exp_ids.append(exp_id)



    def alloc_cluster(self, exp_id):
        if self.empty_cluster_ids:
            cluster_id = self.empty_cluster_ids[0]
            del self.empty_cluster_ids[0]
        else:
            cluster_id = self.next_cluster_id
            self.next_cluster_id += 1
        
        cluster_dict = {"id": cluster_id}
        cluster_dict["members"] = {exp_id}
        x_loglikelihoods = self.exps[exp_id]["x_loglikelihoods"].copy()
        y_loglikelihoods = self.exps[exp_id]["y_loglikelihoods"].copy()
        cluster_dict["x_loglikelihoods"] = x_loglikelihoods
        cluster_dict["y_loglikelihoods"] = y_loglikelihoods
        x_weights = np.exp(x_loglikelihoods - np.max(x_loglikelihoods))
        y_weights = np.exp(y_loglikelihoods - np.max(y_loglikelihoods))
        cluster_dict["x_weights"] = x_weights / np.sum(x_weights)
        cluster_dict["y_weights"] = y_weights / np.sum(y_weights)
        cluster_dict["size"] = 1
        cluster_dict["data_size"] = self.exps[exp_id]["data_size"]
        self.clusters[cluster_id] = cluster_dict
        if cluster_dict["data_size"] >= self.threshold:
            cluster_dict["large"] = True
            cluster_dict["x_kernel_index"] = int(np.argmax(x_weights))
            cluster_dict["y_kernel_index"] = int(np.argmax(y_weights))
            x_coef, y_coef = self.fit_coef(cluster_id)
            cluster_dict["x_coef"] = x_coef
            cluster_dict["y_coef"] = y_coef
        else:
            cluster_dict["large"] = False
            cluster_dict["x_coef"] = np.ones((5, 1), dtype=np.float32)
            cluster_dict["y_coef"] = np.ones((5, 1), dtype=np.float32)
            cluster_dict["x_kernel_index"] = 0
            cluster_dict["y_kernel_index"] = 0

        return cluster_id

    

    def add_exp_to_cluster(self, exp_id, cluster_id):
        cluster = self.clusters[cluster_id]
        cluster["members"].add(exp_id)
        self.exps[exp_id]["cluster"] = cluster_id
        x_loglikelihoods = self.exps[exp_id]["x_loglikelihoods"]
        y_loglikelihoods = self.exps[exp_id]["y_loglikelihoods"]
        cluster["x_loglikelihoods"] += x_loglikelihoods
        cluster["y_loglikelihoods"] += y_loglikelihoods
        new_x_loglikelihoods = cluster["x_loglikelihoods"]
        new_y_loglikelihoods = cluster["y_loglikelihoods"]
        x_weights = np.exp(new_x_loglikelihoods - np.max(new_x_loglikelihoods))
        y_weights = np.exp(new_y_loglikelihoods - np.max(new_y_loglikelihoods))
        cluster["x_weights"] = x_weights / np.sum(x_weights)
        cluster["y_weights"] = y_weights / np.sum(y_weights)
        cluster["size"] += 1
        cluster["data_size"] += self.exps[exp_id]["data_size"]

        if cluster["data_size"] >= self.threshold:
            cluster["large"] = False
            cluster["x_kernel_index"] = int(np.argmax(x_weights))
            cluster["y_kernel_index"] = int(np.argmax(y_weights))
            x_coef, y_coef = self.fit_coef(cluster_id)
            cluster["x_coef"] = x_coef
            cluster["y_coef"] = y_coef



    def remove_exp_from_cluster(self, exp_id, cluster_id):
        cluster = self.clusters[cluster_id]
        if cluster["size"] == 1:
            self.empty_cluster_ids.append(cluster_id)
            del self.clusters[cluster_id]
            return

        cluster["members"].discard(exp_id)
        x_loglikelihoods = self.exps[exp_id]["x_loglikelihoods"]
        y_loglikelihoods = self.exps[exp_id]["y_loglikelihoods"]
        cluster["x_loglikelihoods"] -= x_loglikelihoods
        cluster["y_loglikelihoods"] -= y_loglikelihoods
        new_x_loglikelihoods = cluster["x_loglikelihoods"]
        new_y_loglikelihoods = cluster["y_loglikelihoods"]
        x_weights = np.exp(new_x_loglikelihoods - np.max(new_x_loglikelihoods))
        y_weights = np.exp(new_y_loglikelihoods - np.max(new_y_loglikelihoods))
        cluster["x_weights"] = x_weights / np.sum(x_weights)
        cluster["y_weights"] = y_weights / np.sum(y_weights)
        cluster["size"] -= 1
        cluster["data_size"] -= self.exps[exp_id]["data_size"]

        if cluster["data_size"] >= self.threshold:
            cluster["x_kernel_index"] = int(np.argmax(x_weights))
            cluster["y_kernel_index"] = int(np.argmax(y_weights))
            x_coef, y_coef = self.fit_coef(cluster_id)
            cluster["x_coef"] = x_coef
            cluster["y_coef"] = y_coef
        else:
            cluster["large"] = False


    
    def fit_coef(self, cluster_id):
        cluster = self.clusters[cluster_id]
        x_noisy, _, x_length_scale, x_prior_var, x_noise_ratio = self.x_priors[cluster["x_kernel_index"]]
        y_noisy, _, y_length_scale, y_prior_var, y_noise_ratio = self.y_priors[cluster["y_kernel_index"]]
        x_coors = [self.exps[exp_id]["x_data"][:, :-1] for exp_id in cluster["members"]]
        x_targets = [self.exps[exp_id]["x_data"][:, -1].reshape(-1, 1) for exp_id in cluster["members"]]
        y_coors = [self.exps[exp_id]["y_data"][:, :-1] for exp_id in cluster["members"]]
        y_targets = [self.exps[exp_id]["y_data"][:, -1].reshape(-1, 1) for exp_id in cluster["members"]]

        if x_noisy:
            A = sum([x_coor.T @ x_coor for x_coor in x_coors])
            B = sum([x_coor.T @ x_target for x_coor, x_target in zip(x_coors, x_targets)])
            x_coef = lg.inv(A) @ B
        else:
            inverse_covs = [lg.inv(
                (self.kernel(x_coor, length_scales=x_length_scale) + np.eye(len(x_coor))*x_noise_ratio)*x_prior_var
                ) for x_coor in x_coors]
            A = sum([x_coor.T @ inverse_cov @ x_coor for x_coor, inverse_cov in zip(x_coors, inverse_covs)])
            B = sum([x_coor.T @ inverse_cov @ x_target for x_coor, x_target, inverse_cov in zip(x_coors, x_targets, inverse_covs)])
            x_coef = lg.inv(A) @ B

        if y_noisy:
            A = sum([y_coor.T @ y_coor for y_coor in y_coors])
            B = sum([y_coor.T @ y_target for y_coor, y_target in zip(y_coors, y_targets)])
            y_coef = lg.inv(A) @ B
        else:
            inverse_covs = [lg.inv(
                (self.kernel(y_coor, length_scales=y_length_scale) + np.eye(len(y_coor))*y_noise_ratio)*y_prior_var
                ) for y_coor in y_coors]
            A = sum([y_coor.T @ inverse_cov @ y_coor for y_coor, inverse_cov in zip(y_coors, inverse_covs)])
            B = sum([y_coor.T @ inverse_cov @ y_target for y_coor, y_target, inverse_cov in zip(y_coors, y_targets, inverse_covs)])
            y_coef = lg.inv(A) @ B

        return x_coef, y_coef



    def gibbs_sweep(self, return_log_prob=False):
        for exp_id in self.exps:
            exp = self.exps[exp_id]
            log_probs = np.zeros(len(self.clusters)+1, dtype=np.float32)
            clusters = np.zeros(len(self.clusters)+1, dtype=np.int32)
            log_probs[0] = exp["log_self_prob"] + math.log(self.alpha)
            for index, cluster_id in enumerate(self.clusters, 1):
                solo_cluster, log_prob = self.calculate_likelihood(exp_id, cluster_id)
                if solo_cluster:
                    log_probs[index] = log_probs[0]
                    log_probs[0] = -np.inf
                elif exp["cluster"] == cluster_id:
                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id]["size"] - 1)
                else:
                    log_probs[index] = log_prob + math.log(self.clusters[cluster_id]["size"])

                clusters[index] = cluster_id
            
            log_probs -= np.max(log_probs)
            probs = np.exp(log_probs)
            probs /= np.sum(probs)
            selected_cluster = np.random.choice(clusters, p=probs)
            
            original_cluster_id = exp["cluster"]
            if selected_cluster != original_cluster_id:
                self.remove_exp_from_cluster(exp_id, original_cluster_id)
                if selected_cluster:
                    self.add_exp_to_cluster(exp_id, int(selected_cluster))
                else:
                    exp["cluster"] = self.alloc_cluster(exp_id)
        
        if return_log_prob:
            log_prob = 0
            return log_prob
        


    def calculate_likelihood(self, exp_id, cluster_id):
        exp = self.exps[exp_id]
        cluster = self.clusters[cluster_id]
        solo_cluster = False

        if exp["cluster"] == cluster_id and cluster["size"] == 1:
            solo_cluster = True
            log_prob = 0
        elif exp["cluster"] == cluster_id and (cluster["data_size"] - exp["data_size"]) < self.threshold:
            x_loglikelihoods = cluster["x_loglikelihoods"] - exp["x_loglikelihoods"]
            y_loglikelihoods = cluster["y_loglikelihoods"] - exp["y_loglikelihoods"]

            x_weights = np.exp(x_loglikelihoods - np.max(x_loglikelihoods))
            y_weights = np.exp(y_loglikelihoods - np.max(y_loglikelihoods))
            x_weights /= np.sum(x_weights)
            y_weights /= np.sum(y_weights)

            x_log_scale = np.max(exp["x_loglikelihoods"])
            y_log_scale = np.max(exp["y_loglikelihoods"])
            x_prob = np.sum(x_weights * np.exp(exp["x_loglikelihoods"] - x_log_scale))
            y_prob = np.sum(y_weights * np.exp(exp["y_loglikelihoods"] - y_log_scale))
            prob = x_prob * y_prob
            log_prob = x_log_scale + y_log_scale + (math.log(prob) if prob else -40)

        elif not cluster["large"]:
            x_log_scale = np.max(exp["x_loglikelihoods"])
            y_log_scale = np.max(exp["y_loglikelihoods"])
            x_prob = np.sum(cluster["x_weights"] * np.exp(exp["x_loglikelihoods"] - x_log_scale))
            y_prob = np.sum(cluster["y_weights"] * np.exp(exp["y_loglikelihoods"] - y_log_scale))
            prob = x_prob * y_prob
            log_prob = x_log_scale + y_log_scale + (math.log(prob) if prob else -40)
        else:
            x_noisy, _, x_length_scale, x_prior_var, x_noise_ratio = self.x_priors[cluster["x_kernel_index"]]
            y_noisy, _, y_length_scale, y_prior_var, y_noise_ratio = self.y_priors[cluster["y_kernel_index"]]

            x_coor = self.exps[exp_id]["x_data"][:, :-1]
            x_target = self.exps[exp_id]["x_data"][:, -1].reshape(-1, 1)
            y_coor = self.exps[exp_id]["y_data"][:, :-1]
            y_target = self.exps[exp_id]["y_data"][:, -1].reshape(-1, 1)

            # print(x_target.shape, x_coor.shape)
            x_diff = x_target - x_coor @ cluster["x_coef"]
            y_diff = y_target - y_coor @ cluster["y_coef"]
            data_size = exp["data_size"]

            if x_noisy:
                x_loglikelihood = -0.5*(x_diff / (x_prior_var*x_noise_ratio + x_prior_var)).T @ x_diff - \
                    0.5*data_size*math.log(x_prior_var*x_noise_ratio + x_prior_var)
            else:
                cov = x_prior_var*self.kernel(x_coor, length_scales=x_length_scale) + np.eye(data_size)*x_noise_ratio*x_prior_var
                x_loglikelihood = -0.5*(x_diff.T @ lg.inv(cov) @ x_diff)
                sign, log_det = lg.slogdet(cov)
                log_det = log_det if sign else -100
                x_loglikelihood -= 0.5*log_det

            if y_noisy:
                y_loglikelihood = -0.5*(y_diff / (y_prior_var*y_noise_ratio + y_prior_var)).T @ y_diff - \
                    0.5*exp["data_size"]*math.log(y_prior_var*y_noise_ratio + y_prior_var)
            else:
                cov = y_prior_var*self.kernel(y_coor, length_scales=y_length_scale) + np.eye(data_size)*y_noise_ratio*y_prior_var
                y_loglikelihood = -0.5*(y_diff.T @ lg.inv(cov) @ y_diff)
                sign, log_det = lg.slogdet(cov)
                log_det = log_det if sign else -100
                y_loglikelihood -= 0.5*log_det

            log_prob = x_loglikelihood + y_loglikelihood

        return solo_cluster, log_prob


    def freeze(self):
        self.backup_clusters = {key: copy.deepcopy(value) for key, value in self.clusters.items()}
    


    def MAP(self, rounds=20):
        pass



    def save(self, dir_path=None):
        dir_path = "archive" if dir_path is None else dir_path
        base_configs = {"alpha": self.alpha, 
                        "next_exp_id": self.next_exp_id, 
                        "next_cluster_id": self.next_cluster_id, 
                        "empty_cluster_ids": self.empty_cluster_ids.copy(), 
                        "empty_exp_ids": self.empty_exp_ids.copy(), 
                        "threshold": self.threshold} # <--
        
        exp_seps = [] 
        exp_length = 0
        exps_x_data = []
        exps_y_data = []

        for exp_id, exp in self.exps.items():
            exp_seps.append((exp_id, exp_length, exp_length + exp["data_size"]))
            exp_length += exp["data_size"]
            exps_x_data.append(exp["x_data"])
            exps_y_data.append(exp["y_data"])
        base_configs["exp_seps"] = exp_seps
        
        exps_x_data = np.concatenate(exps_x_data, axis=0)[None, :, :]
        exps_y_data = np.concatenate(exps_y_data, axis=0)[None, :, :]
        exp_data = np.concatenate((exps_x_data, exps_y_data), axis=0) # <--

        clusters_data = {} # <--
        for cluster_id, cluster in self.clusters.items():
            cluster_info = {}
            for key, value in cluster.items():
                if type(value) == np.ndarray:
                    cluster_info[key] = value.tolist()
                elif type(value) == set:
                    cluster_info[key] = list(value)
                else:
                    cluster_info[key] = value
            
            clusters_data[cluster_id] = cluster_info
        
        if not os.path.exists(dir_path):
            os.mkdir(dir_path)

        np.save(os.path.join(dir_path, "exp_data.npy"), exp_data)
        np.save(os.path.join(dir_path, "GP_x.npy"), self.x_prior_data)
        np.save(os.path.join(dir_path, "GP_y.npy"), self.y_prior_data)

        with open(os.path.join(dir_path, "base_configs.json"), "w") as file:
            json.dump(base_configs, file)

        with open(os.path.join(dir_path, "cluster_data.json"), "w") as file:
            json.dump(clusters_data, file)
        


    def load(self, dir_path):
        prior_path = {"x": os.path.join(dir_path, "GP_x.npy"), 
                      "y": os.path.join(dir_path, "GP_y.npy")}
        self.load_priors(prior_path)

        with open(os.path.join(dir_path, "base_configs.json"), "r") as file:
            base_configs = json.load(file)

        with open(os.path.join(dir_path, "cluster_data.json"), "r") as file:
            cluster_data = json.load(file)
        
        exp_data = np.load(os.path.join(dir_path, "exp_data.npy"))

        self.alpha = base_configs["alpha"]
        self.exps = {}
        self.clusters = {}
        self.next_exp_id = base_configs["next_exp_id"]
        self.next_cluster_id = base_configs["next_cluster_id"]
        self.empty_cluster_ids = base_configs["empty_cluster_ids"]
        self.empty_exp_ids = base_configs["empty_exp_ids"]
        self.threshold = base_configs["threshold"]

        exp_seps = base_configs["exp_seps"]
        exps = []
        ids = []

        for exp_id, start, end in exp_seps:
            x_exp, y_exp = exp_data[:, start: end, :].copy()
            exps.append({"x": x_exp, "y": y_exp})
            ids.append(exp_id)

        self.read_experiences(exps, ids)

        for cluster_id, cluster in cluster_data.items():
            updated_dict = {}
            for key, value in cluster.items():
                if key == "members":
                    updated_dict[key] = set(value)
                    for exp_id in value:
                        self.exps[exp_id]["cluster"] = cluster_id
                elif type(value) == list:
                    updated_dict[key] = np.array(value)
            
            cluster.update(updated_dict)
            self.clusters[cluster_id] = cluster
                    


    
# a = np.zeros((2, 4))
# print(type(a))

# print(os.path.exists("coor_x.npy"))



